---
title: "informe2"
author: "Gazze - Landa - Irisarri"
date: "2024-04-23"
output: html_document
---
```{r}
#| label = parametros
set.seed(412)
knitr::opts_chunk$set(fig.align = "center")
```


```{r}
#| label = librerias
library(ggplot2)
library(gridExtra)
library(kableExtra)
library(mvtnorm)
```

# Metropolis Hastings en 1D

## 1

```{r}
d_propuesta_def <- function(x, mean){
  dnorm(x, mean, sd= 1)
}

sample_mh <- function (d_objetivo, r_propuesta, d_propuesta = d_propuesta_def, p_inicial, n){
  stopifnot(n>0)
  muestras <- numeric(n)
  muestras[1]<-p_inicial
  for (i in 2:n) {
    p_actual <- muestras[i-1]
    p_nuevo <- r_propuesta(p_actual)
    f_actual <- d_objetivo(p_actual)
    f_nuevo <- d_objetivo(p_nuevo)
    
    q_actual <- d_propuesta(p_actual,mean=p_nuevo)
    q_nuevo <- d_propuesta(p_nuevo,mean=p_actual)
    
    alpha <- min(1, (f_nuevo/f_actual)*(q_actual/q_nuevo))
    
    aceptar <- rbinom(1,1, alpha)
    if (aceptar){
      muestras[i] <- p_nuevo
    } else {
      muestras[i] <- p_actual
    }
    
  }
  return ( muestras ) 
}
```


## Distribución de Kumaraswamy

## 2

```{r}
d_kumaraswamy <- function(x, a ,b){
  if(x<0 || x>1){
    f=0
  }else{
    f <- a*b*(x^(a-1))*(1-x^a)^(b-1)
  }
  return(f)
}
x<-seq(0,1, length.out= 200)
d_kumaraswamy.5.5 <- numeric(length(x))
d_kumaraswamy51<- numeric(length(x))
d_kumaraswamy13 <-numeric(length(x))
d_kumaraswamy22 <- numeric(length(x))
d_kumaraswamy25 <- numeric(length(x))
for (i in 1:length(x)) {
  d_kumaraswamy.5.5[i] <- d_kumaraswamy(x[i], 0.5, 0.5)
  d_kumaraswamy51[i] <- d_kumaraswamy(x[i], 5, 1)
  d_kumaraswamy13[i] <- d_kumaraswamy(x[i], 1, 3)
  d_kumaraswamy22[i] <- d_kumaraswamy(x[i], 2, 2)
  d_kumaraswamy25[i] <- d_kumaraswamy(x[i], 2, 5)
}


df <- data.frame(x,
                 d_kumaraswamy.5.5,
                 d_kumaraswamy51,
                 d_kumaraswamy13,
                 d_kumaraswamy22,
                 d_kumaraswamy25)
ggplot(df, aes(x = x)) +
  # Agregar las líneas para cada distribución
  geom_line(aes(y = d_kumaraswamy.5.5, color = "d_kumaraswamy.5.5")) +
  geom_line(aes(y = d_kumaraswamy51, color = "d_kumaraswamy51")) +
  geom_line(aes(y = d_kumaraswamy13, color = "d_kumaraswamy13")) +
  geom_line(aes(y = d_kumaraswamy22, color = "d_kumaraswamy22")) +
  geom_line(aes(y = d_kumaraswamy25, color = "d_kumaraswamy25")) +
  # Personalizar aspectos del gráfico
  labs(title = "",
       x = "",
       y = "Densidad") +
  scale_color_manual(values = c("d_kumaraswamy.5.5" = "blue",
                                 "d_kumaraswamy51" = "green",
                                 "d_kumaraswamy13" = "red",
                                 "d_kumaraswamy22" = "cyan",
                                 "d_kumaraswamy25" = "purple"),
                     labels = c("d_kumaraswamy.5.5" = "a = 0.5, b = 0.5",
                                "d_kumaraswamy51" = "a = 5, b = 1",
                                "d_kumaraswamy13" = "a = 1, b = 3",
                                "d_kumaraswamy22" = "a = 2, b = 2",
                                "d_kumaraswamy25" = "a = 2, b = 5")) +
  guides(color=guide_legend(title = "Parámetros"))+
  theme_minimal()
```
La distribución de Kumaraswamy resulta muy útil a la hora de realizar estudios bayesianos, ya que presenta una es gran flexibilidad y puede modelar una amplia gama de formas de distribución de probabilidad.
Ademas presenta una función de densidad de probabilidad simple y computacionalmente eficiente, lo que facilita su uso en cálculos bayesianos.
(Por último, vale destacar que la restricción al intervalo (0,1) de la distribución de Kumaraswamy resulta útil porque se adapta naturalmente al modelado de proporciones y probabilidades datito*ver)

## 3
```{r}
get_beta_pars <- function(mu, kappa){
  alpha <- mu *kappa
  beta <- (1-mu)*kappa
  return(list(alpha = alpha, beta = beta))
}
#graficar kumaraswamy 6 2, y una beta 8,2 para mostrar que son parecidas
d_objetivo <- function(x, a=6 ,b=2){
  if(x<0 || x>1){
    f=0
  }else{
    f <- a*b*(x^(a-1))*(1-x^a)^(b-1)
  }
  return(f)
}
kappas <- c(1,25,50)
n=5000
muestras <- matrix(nrow = n,ncol = length(kappas))
for (i in 1:3) {
  #p_inicial = runif(1) se agrega?
  r_propuesta <- function(x) {
    kappa <- kappas[i]
    pars <- get_beta_pars(x, kappa)
    rbeta(1, pars$alpha, pars$beta)
  }
  d_propuesta <- function(x, mean) {
    kappa <- kappas[i]
    pars <- get_beta_pars(mean, kappa)
    dbeta(x, pars$alpha, pars$beta)
  }
  p_inicial <- runif(1)
  
  muestras[,i] <- sample_mh(d_objetivo, r_propuesta, d_propuesta, p_inicial= p_inicial, n=n)
}


df <- as.data.frame(cbind(muestras, x=1:n))
ggplot(df, aes(x = x)) +
  geom_line(aes(y = V1), color = "#536C8D") +
  geom_line(aes(y = V2), color = "#62A07B") +
  geom_line(aes(y = V3), color = "#613860") +
  labs(x = "", y = "", title = "")

```

```{r}
g1 <- ggplot(df, aes(x = V1)) +
  geom_histogram(color = "black", fill = "#536C8D", bins = 30) +
  labs(x = "", y = "Frecuencia", title = "")+
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 500))

# Graficar histograma para V2
g2 <- ggplot(df, aes(x = V2)) +
  geom_histogram(color = "black", fill = "#62A07B", bins = 30) +
  labs(x = "", y = "Frecuencia", title = "")+
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 500))

# Graficar histograma para V3
g3 <- ggplot(df, aes(x = V3)) +
  geom_histogram(color = "black", fill = "#613860", bins = 30) +
  labs(x = "", y = "Frecuencia", title = "")+
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 500))

# Mostrar los histogramas en una sola columna
grid.arrange(g1, g2, g3, ncol = 1)
```

```{r}
# averiguar si hay grafico para el correlograma

calcular_w <- function(x){
  mean(apply(x, 2, var))
}

calcular_b <- function(x){
  media_chain <- apply(x, 2, mean)
  var(media_chain) * nrow(x)
}

R_hat <- function(x){
  S <- nrow(x)
  M <- ncol(x)
  W <- calcular_w(x)
  B <- calcular_b(x)
  numerador <- (S-1)/S * W + (1 / S) * B
  sqrt(numerador / W)
}

R_hat(muestras)
```

```{r}
corr <- acf(muestras[,3], plot = FALSE, lag = Inf)
N_eff <- function(x){
  n_eff <- numeric(ncol(x))
  for (i in 1:ncol(x)) {
    S <- nrow(x)
    autocorr <- acf(x[,i], plot = FALSE, lag = Inf)$acf
    limite <- which(autocorr<0.05)[1]
    
    denominador <- 1 + 2 * sum(autocorr[2:limite])
    n_eff[i] <- S / denominador  
  }
  return(n_eff)
}
N_eff(muestras)
data_corr = data.frame(x=corr$lag, y=corr$acf)
ggplot(data_corr)+
  aes(x=x,y=y)+
  geom_line()+
  geom_hline(yintercept = 0, linetype = "dashed", color = "#613860")+
  labs(x = "Lag", y = "Autocorrelación", title = "")
# no parecen muy bajos para 5k muestras
# YA GENERALICE LA N_eff 
```


## 4

```{r}
medias <- c(mean(muestras[,1]),mean(muestras[,2]),mean(muestras[,3]))
percentiles<- rbind(quantile(muestras[,1], c(0.05, 0.95)),
                    quantile(muestras[,2], c(0.05, 0.95)),
                    quantile(muestras[,3], c(0.05, 0.95)))

tabla <-cbind(medias, percentiles)
colnames(tabla) <- c("Medias","Percentil 5", "Percentil 95")
rownames(tabla) <- c("Kappa = 1","Kappa = 25", "Kappa = 50")
kable(round(tabla,3))%>% 
  kable_styling(position = "center",full_width = FALSE) %>% 
  row_spec(0, bold = T, background = "#613860", color = "black") 
```

```{r}
muestras_logit <- log(muestras/(1-muestras))
medias_logit <- c(mean(muestras_logit[,1]),mean(muestras_logit[,2]),mean(muestras_logit[,3]))
percentiles_logit <- rbind(quantile(muestras_logit[,1], c(0.05, 0.95)),
                    quantile(muestras_logit[,2], c(0.05, 0.95)),
                    quantile(muestras_logit[,3], c(0.05, 0.95)))

tabla_logit <-cbind(medias_logit, percentiles_logit)
colnames(tabla_logit) <- c("Medias","Percentil 5", "Percentil 95")
rownames(tabla_logit) <- c("Kappa = 1","Kappa = 25", "Kappa = 50")
kable(round(tabla_logit,3))%>% 
  kable_styling(position = "center",full_width = FALSE) %>% 
  row_spec(0, bold = T, background = "#613860", color = "black") 
```

# Metropolis-Hastings en 2D

## 6

```{r}
sample_mh_2d <- function (d_objetivo, r_propuesta, d_propuesta, p_inicial, n, sigma_prop){
  stopifnot(n>0)
  muestras <- matrix(nrow = n, ncol = length(p_inicial))
  muestras[1,]<-p_inicial
  for (i in 2:n) {
    p_actual <- muestras[i-1,]
    p_nuevo <- r_propuesta(p_actual)
    f_actual <- d_objetivo(p_actual)
    f_nuevo <- d_objetivo(p_nuevo)
    
    q_actual <- d_propuesta(p_actual,mean=p_nuevo)
    q_nuevo <- d_propuesta(p_nuevo,mean=p_actual)
    
    alpha <- min(1, (f_nuevo/f_actual)*(q_actual/q_nuevo))
    
    aceptar <- rbinom(1,1, alpha)
    if (aceptar){
      muestras[i,] <- p_nuevo
    } else {
      muestras[i,] <- p_actual
    }
    
  }
  return ( muestras ) 
}


```

## 7

```{r warning=FALSE}
mu_obj <- c(0.4, 0.75)
sigma_obj <- matrix(c(1.35, 0.4, 0.4, 2.4), ncol = 2)

sigma_prop <- diag(2) #preguntar esto

d_objetivo <- function(x) dmvnorm(x, mean = mu_obj, sigma = sigma_obj)
r_propuesta <- function(x) rmvnorm(1, mean = x, sigma = sigma_prop)
d_propuesta <- function(x, mean) dmvnorm(x, mean = mean, sigma = sigma_prop)

df_grilla <- expand.grid(x = seq(-3, 3, length.out = 200), seq(-3, 3, length.out = 200))
df_grilla$z <- d_objetivo(as.matrix(df_grilla))

ggplot(df_grilla)+
  aes(x = x, y = Var2, z = z)+
  stat_contour()+
  labs(title = "",
       x = "X1",
       y = "X2")

muestras_2d <- sample_mh_2d(d_objetivo, r_propuesta, d_propuesta, p_inicial=c(runif(1,-2,3),runif(1,-2,3)), n=5000)

muestras_2d_df <- as.data.frame(muestras_2d)
ggplot(muestras_2d_df, aes(x = V1, y = V2)) +
  geom_density_2d(fill = "skyblue", alpha = 0.7) +
  geom_density2d_filled()+
  #geom_point(data = muestras_2d_df, aes(x = V1, y = V2), color = "cyan2", alpha = 0.05) +
  labs(title = "",
       x = "X1",
       y = "X2") +
  stat_contour(aes(x = x, y = Var2, z = z), data = df_grilla, color = "white")+
  theme_minimal()

# p_inicial = rmvnorm(1,mean=c(1,1)) ?
```


```{r}

########################
plot_trace <- function(x, y){
  n <-length(x)
  df <- data.frame(x = rep(1:n, 2), y = c(x, y), dimension = rep(1:2, each = n))
  ggplot(df)+
    geom_line(aes(x = x, y = y))+
    facet_grid(rows = vars(dimension))
}  

plot_trace(muestras_2d_df$V1, muestras_2d_df$V2)

#Número efectivo de muestras
N_eff(muestras_2d)
```

## 8

```{r}
#| label = probs
probs <- numeric(3)
# x1 > 1 ; x2 < 0
mu_filtradas <- muestras_2d[muestras_2d[,1] > 1 & muestras_2d[,2] < 0,]
probs[1] <- nrow(mu_filtradas) / nrow(muestras_2d)
# x1 > 1 ; x2 > 2 
mu_filtradas <- muestras_2d[muestras_2d[,1] > 1 & muestras_2d[,2] > 2,]
probs[2] <- nrow(mu_filtradas) / nrow(muestras_2d)
# x1 > .4 ; x2 > .75
mu_filtradas <- muestras_2d[muestras_2d[,1] > .4 & muestras_2d[,2] > .75,]
probs[3] <- nrow(mu_filtradas) / nrow(muestras_2d)
probs #no se si esto esta bien gente
```

```{r}
# Monte Carlo generamos aleatorios y hace sumatoria
# Genera un gran número de muestras bivariadas
n <- 5000
muestras_2d_mc <- rmvnorm(n, mean = mu_obj, sigma = sigma_obj)
p_exacta <- numeric(3)
p_mc <- numeric(3)
# x1 > 1 ; x2 < 0
mu_filtradas <- muestras_2d_mc[muestras_2d_mc[,1] > 1 & muestras_2d_mc[,2] < 0,]
p_mc[1] <- nrow(mu_filtradas) / nrow(muestras_2d_mc)
p_exacta[1] <- pmvnorm(lower = c(1, -Inf), upper = c(Inf,0), mean = mu_obj, sigma=sigma_obj)
# x1 > 1 ; x2 > 2 
mu_filtradas <- muestras_2d_mc[muestras_2d_mc[,1] > 1 & muestras_2d_mc[,2] > 2,]
p_mc[2] <- nrow(mu_filtradas) / nrow(muestras_2d_mc)
p_exacta[2] <- pmvnorm(lower = c(1, 2), upper = c(Inf,Inf), mean = mu_obj, sigma=sigma_obj)
# x1 > .4 ; x2 > .75
mu_filtradas <- muestras_2d_mc[muestras_2d_mc[,1] > .4 & muestras_2d_mc[,2] > .75,]
p_mc[3] <- nrow(mu_filtradas) / nrow(muestras_2d_mc)
p_exacta[3] <- pmvnorm(lower = c(.4, .75), upper = c(Inf,Inf), mean = mu_obj, sigma=sigma_obj)

probs
p_mc
round(p_exacta, 4)#no se si esto esta bien gente
```


```{r}
#Método de la grilla
grilla= numeric(3)
grilla[1]=sum(df_grilla$z[df_grilla$x>1 & df_grilla$Var2<0])/ sum(df_grilla$z)
grilla[2]=sum(df_grilla$z[df_grilla$x>1 & df_grilla$Var2>2])/ sum(df_grilla$z)
grilla[3]=sum(df_grilla$z[df_grilla$x>0.4 & df_grilla$Var2>0.75])/ sum(df_grilla$z)

data_probs= cbind(muestras=probs, mc= p_mc, grilla= grilla, exacto= p_exacta)

colnames(data_probs) <- c("Muestras","Montecarlo", "Grilla","Prob. Exacta")
rownames(data_probs) <- c("P(X1> 1, X2< 0)","P(X1> 1, X2> 2)", "P(X1> 0.4, X2> 0.75)")
kable(round(data_probs,4))%>% 
  kable_styling(position = "center",full_width = FALSE) %>% 
  row_spec(0, bold = T, background = "#613860", color = "black") 

```



# Función de Rosenbrock

```{r}
a <- 0.5
b <- 5
sigma_prop1 <- matrix(c(4,1,1,4), ncol = 2)
sigma_prop2 <- diag(2)
sigma_prop3 <- matrix(c(2,-2,-2,5), ncol = 2)

d_objetivo <- function(x) exp(-((a-x[1])^2+b*(x[2]-(x[1])^2)^2))
r_propuesta <- function(x) rmvnorm(1, mean = x, sigma = sigma_prop2)
d_propuesta <- function(x, mean) dmvnorm(x, mean = mean, sigma = sigma_prop2)

df_grilla <- expand.grid(x = seq(-2, 2, length.out = 200), seq(-1, 5, length.out = 200))
for(i in 1:40000){ 
df_grilla$z[i] <- d_objetivo(c(df_grilla$x[i], df_grilla$Var2[i]))
}

ggplot(df_grilla)+
 aes(x = x, y = Var2, z = z)+
 stat_contour()+
 labs(title = "",
      x = "X1",
      y = "X2")

muestras_2d2 <- sample_mh_2d(d_objetivo, r_propuesta, d_propuesta, p_inicial= c(runif(1,0,1), runif(1,-0.5, 1)), n=5000)

muestras_2d2_df <- as.data.frame(muestras_2d2)
ggplot(muestras_2d2_df, aes(x = V1, y = V2)) +
  geom_density_2d(fill = "skyblue", alpha = 0.7) +
  geom_density2d_filled()+
  #geom_point(data = muestras_2d_df, aes(x = V1, y = V2), color = "cyan2", alpha = 0.05) +
  labs(title = "",
       x = "X1",
       y = "X2") +
  theme_minimal()

r_propuesta <- function(x) rmvnorm(1, mean = x, sigma = sigma_prop1)
d_propuesta <- function(x, mean) dmvnorm(x, mean = mean, sigma = sigma_prop1)

muestras_2d1 <- sample_mh_2d(d_objetivo, r_propuesta, d_propuesta, p_inicial= c(runif(1,0,1), runif(1,-0.5, 1)), n=5000)

muestras_2d1_df <- as.data.frame(muestras_2d1)
ggplot(muestras_2d1_df, aes(x = V1, y = V2)) +
  geom_density_2d(fill = "skyblue", alpha = 0.7) +
  geom_density2d_filled()+
  #geom_point(data = muestras_2d_df, aes(x = V1, y = V2), color = "cyan2", alpha = 0.05) +
  labs(title = "",
       x = "X1",
       y = "X2") +
  theme_minimal()

r_propuesta <- function(x) rmvnorm(1, mean = x, sigma = sigma_prop3)
d_propuesta <- function(x, mean) dmvnorm(x, mean = mean, sigma = sigma_prop3)

muestras_2d3 <- sample_mh_2d(d_objetivo, r_propuesta, d_propuesta, p_inicial= c(runif(1,0,1), runif(1,-0.5, 1)), n=5000)

muestras_2d3_df <- as.data.frame(muestras_2d3)
ggplot(muestras_2d3_df, aes(x = V1, y = V2)) +
  geom_density_2d(fill = "skyblue", alpha = 0.7) +
  geom_density2d_filled()+
  #geom_point(data = muestras_2d_df, aes(x = V1, y = V2), color = "cyan2", alpha = 0.05) +
  labs(title = "",
       x = "X1",
       y = "X2") +
  theme_minimal()
# TODO: preguntarle a los chicos que cambia en estas tres para poner todo en un bucle
```

```{r}
# Añadir una columna para indicar la cadena
muestras_2d1_df$cadena <- "Muestra 1"
muestras_2d2_df$cadena <- "Muestra 2"
muestras_2d3_df$cadena <- "Muestra 3"

muestra1 <- muestras_2d1_df
muestra2 <- muestras_2d2_df
muestra3 <- muestras_2d3_df
#TODO: esto esta de más, preguntar si se puede sacar

# Unir las muestras en un solo dataframe
df <- rbind(muestra1, muestra2, muestra3)

# Graficar las trayectorias
ggplot(df, aes(x = V1, y = V2, color = cadena)) +
  geom_point() +
  labs(x = "X1", y = "X2", title = "Trayectorias de las cadenas de Metropolis-Hastings")+
  scale_color_manual(values= c("#536C8D","#62A07B","#613860"))
```
```{r}
#Trace plot y gráfico de autocorrelación
# TODO: yo mantendria los colores de muestra 1 2 y 3 como arriba
n=5000
df1 <- as.data.frame(cbind(muestras_2d1_df$V1,muestras_2d2_df$V1,muestras_2d3_df$V1, x=1:n))
tp1=ggplot(df1, aes(x=x)) +
  geom_line(aes(y = V1), color = "#536C8D") +
  geom_line(aes(y = V2), color = "#62A07B") +
  geom_line(aes(y = V3), color = "#613860") +
  labs(x = "X1", y = "", title = "")

df2 <- as.data.frame(cbind(muestras_2d1_df$V2,muestras_2d2_df$V2,muestras_2d3_df$V2, x=1:n))
tp2=ggplot(df2, aes(x=x)) +
  geom_line(aes(y = V1), color = "#536C8D") +
  geom_line(aes(y = V2), color = "#62A07B") +
  geom_line(aes(y = V3), color = "#613860") +
  labs(x = "X2", y = "", title = "")

grid.arrange(tp1, tp2, ncol=1)

corr11 <- acf(muestras_2d1_df[,1], plot = FALSE, lag = Inf)
corr12 <- acf(muestras_2d1_df[,2], plot = FALSE, lag = Inf)
corr21 <- acf(muestras_2d2_df[,1], plot = FALSE, lag = Inf)
corr22 <- acf(muestras_2d2_df[,2], plot = FALSE, lag = Inf)
corr31 <- acf(muestras_2d3_df[,1], plot = FALSE, lag = Inf)
corr32 <- acf(muestras_2d3_df[,2], plot = FALSE, lag = Inf)

corr_x1=data.frame(lag= c(corr11$lag, corr21$lag, corr31$lag),
           acf= c(corr11$acf, corr21$acf, corr31$acf),
           cadena= rep(c("Muestra 1", "Muestra 2", "Muestra 3"), each=5000))

corr_x2=data.frame(lag= c(corr12$lag, corr22$lag, corr32$lag),
           acf= c(corr12$acf, corr22$acf, corr32$acf),
           cadena= rep(c("Muestra 1", "Muestra 2", "Muestra 3"), each=5000))

grafico_corr_x1= ggplot(corr_x1)+
  aes(x=lag,y=acf, color=cadena)+
  geom_line()+
  labs(x = "Lag", y = "Autocorrelación", title = "")+
  scale_color_manual(values= c("#536C8D","#62A07B","#613860"))

grafico_corr_x2= ggplot(corr_x2)+
  aes(x=lag,y=acf, color=cadena)+
  geom_line()+
  labs(x = "Lag", y = "Autocorrelación", title = "")+
  scale_color_manual(values= c("#536C8D","#62A07B","#613860"))

grid.arrange(grafico_corr_x1,grafico_corr_x2, ncol=1)

#Calculamos el n eff de muestras para ver que muestra resulto mejor
round(N_eff(muestras_2d1),2)
round(N_eff(muestras_2d2),2)
round(N_eff(muestras_2d3),2)
```



```{r}
# Ej 10
# Calculamos las probabilidades con la segunda muestra, la cual presenta el mayor número efectivo de muestras
probs <- numeric(3)
# 0 < x1 < 1  ;0 < x2 < 1
mu_filtradas <- muestras_2d2[muestras_2d2[,1] > 0 & muestras_2d2[,1] < 1 & muestras_2d2[,2] > 0 & muestras_2d2[,2] < 1,]
probs[1] <- nrow(mu_filtradas) / nrow(muestras_2d2)
# -1 < x1 < 0  ;0 < x2 < 1 
mu_filtradas <- muestras_2d2[muestras_2d2[,1] > -1 & muestras_2d2[,1] < 0 & muestras_2d2[,2] > 0 & muestras_2d2[,2] < 1,]
probs[2] <- nrow(mu_filtradas) / nrow(muestras_2d2)
# 1 < x1 < 2 ;2 < x2 < 3
mu_filtradas <- muestras_2d2[muestras_2d2[,1] > 1 & muestras_2d2[,1] < 2 & muestras_2d2[,2] > 2 & muestras_2d2[,2] < 3,]
probs[3] <- nrow(mu_filtradas) / nrow(muestras_2d2)
probs
```

```{r}
#Ej 11
#Método de la grilla
grilla= numeric(3)
grilla[1]=sum(df_grilla$z[df_grilla$x>0 & df_grilla$x<1 & df_grilla$Var2>0 & df_grilla$Var2<1])/ sum(df_grilla$z)
grilla[2]=sum(df_grilla$z[df_grilla$x>-1 & df_grilla$x<0 & df_grilla$Var2>0 & df_grilla$Var2<1])/ sum(df_grilla$z)
grilla[3]=sum(df_grilla$z[df_grilla$x>1 & df_grilla$x<2 & df_grilla$Var2>2 & df_grilla$Var2<3])/ sum(df_grilla$z)

data_probs= cbind(muestras=probs,grilla= grilla)

colnames(data_probs) <- c("Muestras","Grilla")
rownames(data_probs) <- c("P(0< X1 < 1,0 < X2< 1)","P(-1 < X1 < 0,0 < X2 < 1)", "P(1 < X1 < 2,2 < X2 < 3)")
kable(round(data_probs,4))%>% 
  kable_styling(position = "center",full_width = FALSE) %>% 
  row_spec(0, bold = T, background = "#613860", color = "black") 
```

Básicamente, a lo largo del trabajo se ve que Metropolis Hastings es una variante que permite sacar muestras de distribuciones de probabilidad, incluso cuando no se tiene idea de cómo es exactamente esa distribución. Se probó en distribuciones unidimensionales y bidimensionales como lo son las distribuciones de Kumaraswamy y Rosenbrock. Se interactuó con diferentes formas de distribución propuesta y se observó cómo esta afecta a la eficiencia del algoritmo. 
Es interesante ver a Metropolis Hastings como una herramienta en estadística bayesiana ya que ayuda a muestrear y/o graficar distribuciones a posteriori desconocidas.


