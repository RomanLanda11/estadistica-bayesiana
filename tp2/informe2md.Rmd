---
title: "informe2"
author: "Gazze - Landa - Irisarri"
date: "2024-04-23"
output: html_document
---
```{r, echo=FALSE}
#| label = parametros
set.seed(412)
knitr::opts_chunk$set(fig.align = "center")
```


```{r, echo=FALSE}
#| label = librerias
library(ggplot2)
library(dplyr)
library(gridExtra)
library(kableExtra)
library(mvtnorm)
```

# Introducción

Metropolis-Hastings es un proceso para generar muestras pseudoaleatorias de distribuciones objetivo. Se trata de un proceso iterativo que comienza en un punto inicial en el espacio de muestras, luego, propone un "salto" a un nuevo punto utilizando una distribución de probabilidad propuesta. Este salto es evaluado mediante un criterio de aceptación basado en la relación entre la densidad en el nuevo punto y en el punto actual. Si el salto es aceptado, el algoritmo se mueve al nuevo punto, sino, permanece en el punto actual. 
Este proceso se repite muchas veces para explorar y muestrear de manera eficiente. En este trabajo práctico, se explorará la aplicabilidad y el rendimiento del algoritmo para distintas distribuciones objetivo y propuestas.

# Objetivo

Se propone explorar y comprender el funcionamiento y la utilidad del algoritmo de Metropolis-Hastings en diferentes situaciones. Se abordarán ejercicios que van desde la generación de muestras unidimensionales, hasta la aplicación del algoritmo en problemas más complejos como la generación de muestras de una distribución normal bivariada y la estimación de la distribución a posteriori de una funciones de densidad complejas. 
A través de estos ejercicios, se buscará evaluar el desempeño del algoritmo y comprender sus ventajas y limitaciones en la generación de muestras de distribuciones de probabilidad.


# Metropolis Hastings en 1D

Para entrar en calor, se comienza declarando una función que implemente el algoritmo de Metropolis-Hastings para tomar muestras de una distribución de probabilidad unidimensional. El algoritmo permiete elegir entre un punto de inicio arbitrario o al azar y utiliza la distribución propuesta de transición arbitraria (por defecto, se utiliza una distribución normal estándar).

## 1. Algoritmo MH en 1 D

```{r}
d_propuesta_def <- function(x, mean){
  dnorm(x, mean, sd= 1)
}
# TODO: agregar tasa de aceptación a la funcion
sample_mh <- function (d_objetivo, r_propuesta, d_propuesta = d_propuesta_def, p_inicial, n){
  stopifnot(n>0)
  salto = 0
  muestras <- numeric(n)
  muestras[1]<-p_inicial
  for (i in 2:n) {
    p_actual <- muestras[i-1]
    p_nuevo <- r_propuesta(p_actual)
    f_actual <- d_objetivo(p_actual)
    f_nuevo <- d_objetivo(p_nuevo)
    
    q_actual <- d_propuesta(p_actual,mean=p_nuevo)
    q_nuevo <- d_propuesta(p_nuevo,mean=p_actual)
    
    alpha <- min(1, (f_nuevo/f_actual)*(q_actual/q_nuevo))
    
    aceptar <- rbinom(1,1, alpha)
    if (aceptar){
      muestras[i] <- p_nuevo
      salto = salto + 1  # TODO: agregar tasa de aceptación a la funcion
    } else {
      muestras[i] <- p_actual
    }
    
  }
  tasa_acepta = (salto/n)*100# TODO: agregar tasa de aceptación a la funcion
  print( tasa_acepta)
  return ( muestras ) 
}
```


# Distribución de Kumaraswamy

La distribución de Kumaraswamy es una distribución de probabilidad continua que se utiliza para modelar variables aleatorias con soporte en el intervalo $(0;1)$.

## 2. Conociendo a Jagdish Kumaraswamy

Se grafica la función de densidad de la distribución de Kumaraswamy para 5 combinaciones de los parámetros $a$ y $b$ con el objetivo de familiarizarse con su comportamiento.

```{r}
d_kumaraswamy <- function(x, a ,b){
  if(x<0 || x>1){
    f=0
  }else{
    f <- a*b*(x^(a-1))*(1-x^a)^(b-1)
  }
  return(f)
}
```


```{r, echo=FALSE}
x<-seq(0,1, length.out= 200)
d_kumaraswamy.5.5 <- numeric(length(x))
d_kumaraswamy51<- numeric(length(x))
d_kumaraswamy13 <-numeric(length(x))
d_kumaraswamy22 <- numeric(length(x))
d_kumaraswamy25 <- numeric(length(x))
for (i in 1:length(x)) {
  d_kumaraswamy.5.5[i] <- d_kumaraswamy(x[i], 0.5, 0.5)
  d_kumaraswamy51[i] <- d_kumaraswamy(x[i], 5, 1)
  d_kumaraswamy13[i] <- d_kumaraswamy(x[i], 1, 3)
  d_kumaraswamy22[i] <- d_kumaraswamy(x[i], 2, 2)
  d_kumaraswamy25[i] <- d_kumaraswamy(x[i], 2, 5)
}


df <- data.frame(x,
                 d_kumaraswamy.5.5,
                 d_kumaraswamy51,
                 d_kumaraswamy13,
                 d_kumaraswamy22,
                 d_kumaraswamy25)
ggplot(df, aes(x = x)) +
  # Agregar las líneas para cada distribución
  geom_line(aes(y = d_kumaraswamy.5.5, color = "d_kumaraswamy.5.5")) +
  geom_line(aes(y = d_kumaraswamy51, color = "d_kumaraswamy51")) +
  geom_line(aes(y = d_kumaraswamy13, color = "d_kumaraswamy13")) +
  geom_line(aes(y = d_kumaraswamy22, color = "d_kumaraswamy22")) +
  geom_line(aes(y = d_kumaraswamy25, color = "d_kumaraswamy25")) +
  # Personalizar aspectos del gráfico
  labs(title = "",
       x = "",
       y = "Densidad") +
  scale_color_manual(values = c("d_kumaraswamy.5.5" = "blue",
                                 "d_kumaraswamy51" = "green",
                                 "d_kumaraswamy13" = "red",
                                 "d_kumaraswamy22" = "cyan",
                                 "d_kumaraswamy25" = "purple"),
                     labels = c("d_kumaraswamy.5.5" = "a = 0.5, b = 0.5",
                                "d_kumaraswamy51" = "a = 5, b = 1",
                                "d_kumaraswamy13" = "a = 1, b = 3",
                                "d_kumaraswamy22" = "a = 2, b = 2",
                                "d_kumaraswamy25" = "a = 2, b = 5")) +
  guides(color=guide_legend(title = "Parámetros"))+
  theme_minimal()
```
La distribución de Kumaraswamy resulta muy útil a la hora de realizar estudios bayesianos, ya que presenta una es gran flexibilidad y puede modelar una amplia gama de formas de distribución de probabilidad.
Ademas presenta una función de densidad de probabilidad simple y computacionalmente eficiente, lo que facilita su uso en cálculos bayesianos.
Por último, vale destacar que la restricción al intervalo $(0;1)$ resulta útil porque se adapta naturalmente al modelado de proporciones y probabilidades.

## 3. MH para Jagdish

Se utiliza la función construida en el punto 1, obtenga 5000 muestras de una distribución de Kumaraswamy con parámetros $a=6 , b=2$ y una distribución de propuesta beta.

```{r}
get_beta_pars <- function(mu, kappa){
  alpha <- mu *kappa
  beta <- (1-mu)*kappa
  return(list(alpha = alpha, beta = beta))
}
#graficar kumaraswamy 6 2, y una beta 8,2 para mostrar que son parecidas
d_objetivo <- function(x, a=6 ,b=2){
  if(x<0 || x>1){
    f=0
  }else{
    f <- a*b*(x^(a-1))*(1-x^a)^(b-1)
  }
  return(f)
}
```

Se observan las cadenas obtenidas utilizando tres grados de concentración distintos $kappas = 1$, $25$ y $50$

```{r, echo=FALSE}
kappas <- c(1,25,50)
n=5000
muestras <- matrix(nrow = n,ncol = length(kappas))
for (i in 1:3) {
  #p_inicial = runif(1) se agrega?
  r_propuesta <- function(x) {
    kappa <- kappas[i]
    pars <- get_beta_pars(x, kappa)
    rbeta(1, pars$alpha, pars$beta)
  }
  d_propuesta <- function(x, mean) {
    kappa <- kappas[i]
    pars <- get_beta_pars(mean, kappa)
    dbeta(x, pars$alpha, pars$beta)
  }
  p_inicial <- runif(1)
  
  muestras[,i] <- sample_mh(d_objetivo, r_propuesta, d_propuesta, p_inicial= p_inicial, n=n)
}


df <- as.data.frame(cbind(muestras, x=1:n))
ggplot(df, aes(x = x)) +
  geom_line(aes(y = V1), color = "#536C8D") +
  geom_line(aes(y = V2), color = "#62A07B") +
  geom_line(aes(y = V3), color = "#613860") +
  labs(x = "", y = "", title = "")

```

```{r, echo=FALSE}
g1 <- ggplot(df, aes(x = V1)) +
  geom_histogram(color = "black", fill = "#536C8D", bins = 30) +
  labs(x = "", y = "Frecuencia", title = "")+
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 500))

# Graficar histograma para V2
g2 <- ggplot(df, aes(x = V2)) +
  geom_histogram(color = "black", fill = "#62A07B", bins = 30) +
  labs(x = "", y = "Frecuencia", title = "")+
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 500))

# Graficar histograma para V3
g3 <- ggplot(df, aes(x = V3)) +
  geom_histogram(color = "black", fill = "#613860", bins = 30) +
  labs(x = "", y = "Frecuencia", title = "")+
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 500))

# Mostrar los histogramas en una sola columna
grid.arrange(g1, g2, g3, ncol = 1)
```

```{r}
calcular_w <- function(x){
  mean(apply(x, 2, var))
}
calcular_b <- function(x){
  media_chain <- apply(x, 2, mean)
  var(media_chain) * nrow(x)
}
R_hat <- function(x){
  S <- nrow(x)
  M <- ncol(x)
  W <- calcular_w(x)
  B <- calcular_b(x)
  numerador <- (S-1)/S * W + (1 / S) * B
  sqrt(numerador / W)
}
R_hat(muestras) #indica convergencia entre cadenas
```


```{r}
corr <- acf(muestras[,3], plot = FALSE, lag = Inf)
N_eff <- function(x){
  n_eff <- numeric(ncol(x))
  for (i in 1:ncol(x)) {
    S <- nrow(x)
    autocorr <- acf(x[,i], plot = FALSE, lag = Inf)$acf
    limite <- which(autocorr<0.05)[1]
    
    denominador <- 1 + 2 * sum(autocorr[2:limite])
    n_eff[i] <- S / denominador  
  }
  return(n_eff)
}
N_eff(muestras)
```


```{r, echo=FALSE}
data_corr = data.frame(x=corr$lag, y=corr$acf)
ggplot(data_corr)+
  aes(x=x,y=y)+
  geom_line()+
  geom_hline(yintercept = 0, linetype = "dashed", color = "#613860")+
  labs(x = "Lag", y = "Autocorrelación", title = "")
# no parecen muy bajos para 5k muestras
```

Lo primero que se puede mencionar es la diferencia en las tasas de aceptación, las cuales fueron $ 21.72$ %, $77.9$% y $81.66$% y los números efectivos de muestras $522.36$, $345.08$ y $133.14$ respectivamente. En cuanto a los recorridos de las muestras, se observa aleatoridad o un patrón de *ruido blanco*. Finalmente observamos el $\hat{R}$ el cual resultó ser $1.003404$ lo que nos indica converngencia entren cadenas.
Tambien, se presentan histogramas de frecuencias observadas para las distintas muestras y se obaservan comportamientos similares para las tres concentraciones elegidas.

## 4. Explorando cadenas

Se presenta una tabla con la media de la distribución y los percentiles 5 y 95 de cada una de las cadenas obtenidas

```{r, echo=FALSE}
medias <- c(mean(muestras[,1]),mean(muestras[,2]),mean(muestras[,3]))
percentiles<- rbind(quantile(muestras[,1], c(0.05, 0.95)),
                    quantile(muestras[,2], c(0.05, 0.95)),
                    quantile(muestras[,3], c(0.05, 0.95)))

tabla <-cbind(medias, percentiles)
colnames(tabla) <- c("Medias","Percentil 5", "Percentil 95")
rownames(tabla) <- c("Kappa = 1","Kappa = 25", "Kappa = 50")
kable(round(tabla,3))%>% 
  kable_styling(position = "center",full_width = FALSE) %>% 
  row_spec(0, bold = T, background = "#613860", color = "black") 
```

Se replica la misma tabla para el $logit(x)$ de las muestras. 

```{r, echo=FALSE}
muestras_logit <- log(muestras/(1-muestras))
medias_logit <- c(mean(muestras_logit[,1]),mean(muestras_logit[,2]),mean(muestras_logit[,3]))
percentiles_logit <- rbind(quantile(muestras_logit[,1], c(0.05, 0.95)),
                    quantile(muestras_logit[,2], c(0.05, 0.95)),
                    quantile(muestras_logit[,3], c(0.05, 0.95)))

tabla_logit <-cbind(medias_logit, percentiles_logit)
colnames(tabla_logit) <- c("Medias","Percentil 5", "Percentil 95")
rownames(tabla_logit) <- c("Kappa = 1","Kappa = 25", "Kappa = 50")
kable(round(tabla_logit,3))%>% 
  kable_styling(position = "center",full_width = FALSE) %>% 
  row_spec(0, bold = T, background = "#613860", color = "black") 
```

# Metropolis-Hastings en 2D

Para extender el algoritmo de Metropolis-Hastings se generaliza la función permitiendole implementar el proceso para más de una dimensión. 
Se plantea una probabilidad de salto normal bivariada de matriz de covarianza variable y se otorga flexibilidad al algoritmo haciendo que reciba como argumento la matriz de covarianza de la probabilidad de transición.

```{r}
sample_mh_2d <- function (d_objetivo, r_propuesta, d_propuesta, p_inicial, n, sigma_prop){
  stopifnot(n>0)
  salto = 0
  muestras <- matrix(nrow = n, ncol = length(p_inicial))
  muestras[1,]<-p_inicial
  for (i in 2:n) {
    p_actual <- muestras[i-1,]
    p_nuevo <- r_propuesta(p_actual)
    f_actual <- d_objetivo(p_actual)
    f_nuevo <- d_objetivo(p_nuevo)
    
    q_actual <- d_propuesta(p_actual,mean=p_nuevo)
    q_nuevo <- d_propuesta(p_nuevo,mean=p_actual)
    
    alpha <- min(1, (f_nuevo/f_actual)*(q_actual/q_nuevo))
    
    aceptar <- rbinom(1,1, alpha)
    if (aceptar){
      muestras[i,] <- p_nuevo
      salto = salto + 1
    } else {
      muestras[i,] <- p_actual
    }
    
  }
  tasa_acepta <- (salto/n)*100
  print(tasa_acepta)
  return ( muestras ) 
}
```

## 7. Conociendo a MH en 2D

Para comenzar a explorar el comportamiento del algoritmo en dos dimensiones se obtendran muestras de una distribución normal bivariada con media $$ \mu = \begin{pmatrix}
   0.4 \\
   0.75
\end{pmatrix} $$
 y matriz de covarianza $$\Sigma=\begin{pmatrix}
   0.35 & 0.4 \\
   0.4 & 2.4
\end{pmatrix} $$

Se comienza graficando la distribución objetivo para seleccionar un punto inicial contenido en el recorrido de la misma.

```{r warning=FALSE, echo=FALSE}
mu_obj <- c(0.4, 0.75)
sigma_obj <- matrix(c(1.35, 0.4, 0.4, 2.4), ncol = 2)

sigma_prop <- diag(2) #preguntar esto

d_objetivo <- function(x) dmvnorm(x, mean = mu_obj, sigma = sigma_obj)
r_propuesta <- function(x) rmvnorm(1, mean = x, sigma = sigma_prop)
d_propuesta <- function(x, mean) dmvnorm(x, mean = mean, sigma = sigma_prop)

df_grilla <- expand.grid(x = seq(-3, 3, length.out = 200), seq(-3, 4, length.out = 200))
df_grilla$z <- d_objetivo(as.matrix(df_grilla))

ggplot(df_grilla)+
  aes(x = x, y = Var2, z = z)+
  stat_contour()+
  labs(title = "",
       x = "X1",
       y = "X2")

```
Luego se utiliza la función **sample_mh_2d** para obtener cinco mil muestras de esta distribución y se grafican.

```{r warning=FALSE, echo=FALSE}
set.seed(412)
muestras_2d <- sample_mh_2d(d_objetivo, r_propuesta, d_propuesta, p_inicial=c(runif(1,-2,3),runif(1,-2,3)), n=5000)

muestras_2d_df <- as.data.frame(muestras_2d)
ggplot(muestras_2d_df, aes(x = V1, y = V2)) +
  geom_density_2d(fill = "skyblue", alpha = 0.7) +
  geom_density2d_filled()+
  geom_point(data = muestras_2d_df, aes(x = V1, y = V2), color = "white", alpha = 0.03) +
  labs(title = "",
       x = "X1",
       y = "X2") +
  stat_contour(aes(x = x, y = Var2, z = z), data = df_grilla, color = "white")+
  theme_minimal()
```

```{r}
plot_trace <- function(x, y){
  n <-length(x)
  df <- data.frame(x = rep(1:n, 2), y = c(x, y), dimension = rep(1:2, each = n))
  ggplot(df)+
    geom_line(aes(x = x, y = y))+
    facet_grid(rows = vars(dimension))+
    labs(x = "Número de Muestra", y = "Valor")
}  
plot_trace(muestras_2d_df$V1, muestras_2d_df$V2)
#Número efectivo de muestras
N_eff(muestras_2d)
```
Se observa nuevamente un patrón de *ruido blanco* en los Trace Plot, lo cual indica una mustra poco correlacionada a través de las observaciones. Mientras que el número efectivo de muestras para $X_1$ es igual a $481.98$ y para $X_2$ resulta $363.31$, lo cual indica que nuestras cinco mil muestras correlacionadas equivalen a cuatrocientas ochenta y dos y trecientas secenta y tres muestras independientes, respectivamente. 

A fines de comparar la calidad de las muestras obtenidas, se calculan y comparan algunas probabilidades mediante distitntos metodos.

```{r, echo=FALSE}
#| label = probs
probs <- numeric(3)
# x1 > 1 ; x2 < 0
mu_filtradas <- muestras_2d[muestras_2d[,1] > 1 & muestras_2d[,2] < 0,]
probs[1] <- nrow(mu_filtradas) / nrow(muestras_2d)
# x1 > 1 ; x2 > 2 
mu_filtradas <- muestras_2d[muestras_2d[,1] > 1 & muestras_2d[,2] > 2,]
probs[2] <- nrow(mu_filtradas) / nrow(muestras_2d)
# x1 > .4 ; x2 > .75
mu_filtradas <- muestras_2d[muestras_2d[,1] > .4 & muestras_2d[,2] > .75,]
probs[3] <- nrow(mu_filtradas) / nrow(muestras_2d)
```

```{r, echo=FALSE}
# Monte Carlo generamos aleatorios y hace sumatoria
# Genera un gran número de muestras bivariadas
n <- 5000
muestras_2d_mc <- rmvnorm(n, mean = mu_obj, sigma = sigma_obj)
p_exacta <- numeric(3)
p_mc <- numeric(3)
# x1 > 1 ; x2 < 0
mu_filtradas <- muestras_2d_mc[muestras_2d_mc[,1] > 1 & muestras_2d_mc[,2] < 0,]
p_mc[1] <- nrow(mu_filtradas) / nrow(muestras_2d_mc)
p_exacta[1] <- pmvnorm(lower = c(1, -Inf), upper = c(Inf,0), mean = mu_obj, sigma=sigma_obj)
# x1 > 1 ; x2 > 2 
mu_filtradas <- muestras_2d_mc[muestras_2d_mc[,1] > 1 & muestras_2d_mc[,2] > 2,]
p_mc[2] <- nrow(mu_filtradas) / nrow(muestras_2d_mc)
p_exacta[2] <- pmvnorm(lower = c(1, 2), upper = c(Inf,Inf), mean = mu_obj, sigma=sigma_obj)
# x1 > .4 ; x2 > .75
mu_filtradas <- muestras_2d_mc[muestras_2d_mc[,1] > .4 & muestras_2d_mc[,2] > .75,]
p_mc[3] <- nrow(mu_filtradas) / nrow(muestras_2d_mc)
p_exacta[3] <- pmvnorm(lower = c(.4, .75), upper = c(Inf,Inf), mean = mu_obj, sigma=sigma_obj)
```


```{r}
#Método de la grilla
grilla= numeric(3)
grilla[1]=sum(df_grilla$z[df_grilla$x>1 & df_grilla$Var2<0])/ sum(df_grilla$z)
grilla[2]=sum(df_grilla$z[df_grilla$x>1 & df_grilla$Var2>2])/ sum(df_grilla$z)
grilla[3]=sum(df_grilla$z[df_grilla$x>0.4 & df_grilla$Var2>0.75])/ sum(df_grilla$z)

data_probs= cbind(muestras=probs, mc= p_mc, grilla= grilla, exacto= p_exacta)

colnames(data_probs) <- c("Muestras","Montecarlo", "Grilla","Prob. Exacta")
rownames(data_probs) <- c("P(X1> 1, X2< 0)","P(X1> 1, X2> 2)", "P(X1> 0.4, X2> 0.75)")
kable(round(data_probs,4))%>% 
  kable_styling(position = "center",full_width = FALSE) %>% 
  row_spec(0, bold = T, background = "#613860", color = "black") 

```
Como se observa en la tabla, si bien las probabilidades obtenidas mediante el metodo de Monte Carlo son las que más se aproximan a las reales, con el método de Metropolis Hasting se obtienen muy buenas aproximaciones. Vale aclarar que en situaciones más complejas, donde se dificulta el calculo de las probabilidades exactas, la aplicación de MH toma mayor importancia.


# Función de Rosenbrock

La función de Rosenbrock, comunmente conocida como la “banana de Rosenbrock”, es una función matemática utilizada frecuentemente como un problema de optimización y prueba para algoritmos de optimización numérica. También es muy conocida en el campo de la estadística bayesiana, ya que en ciertos escenarios, la densidad del posterior toma una forma que definitivamente se asemeja a la banana de Howard Rosenbrock.

Un ejemplo de esto se presenta a continuación:
$$p^*(x_1, x_2 \mid a, b) = \exp \left\{-\left[(a - x_1) ^ 2 + b(x_2 - x_1^2) ^ 2\right] \right\}$$
El objetivo en este tramo del trabajo es obtener muestras de dicha función utilizando el algoritmo de Metropolis Hasting, para esto, se comienza graficandola para conocer su recorrido y asi poder seleccionar un punto inicial.

```{r, echo=FALSE}
set.seed(412)
a <- 0.5
b <- 5
sigma_prop1 <- matrix(c(4,1,1,4), ncol = 2)
sigma_prop2 <- diag(2)*1
sigma_prop3 <- matrix(c(2,-2,-2,5), ncol = 2)

d_objetivo <- function(x) exp(-((a-x[1])^2+b*(x[2]-(x[1])^2)^2))
r_propuesta <- function(x) rmvnorm(1, mean = x, sigma = sigma_prop1)
d_propuesta <- function(x, mean) dmvnorm(x, mean = mean, sigma = sigma_prop1)

df_grilla <- expand.grid(x = seq(-2, 2, length.out = 200), seq(-1, 5, length.out = 200))
for(i in 1:40000){ 
df_grilla$z[i] <- d_objetivo(c(df_grilla$x[i], df_grilla$Var2[i]))
}

ggplot(df_grilla)+
 aes(x = x, y = Var2, z = z)+
 stat_contour()+
 labs(title = "",
      x = "X1",
      y = "X2")
```

```{r, echo=FALSE}
muestras_2d1 <- sample_mh_2d(d_objetivo, r_propuesta, d_propuesta, p_inicial= c(runif(1,0,1), runif(1,-0.5, 1)), n=5000)
muestras_2d1_df <- as.data.frame(muestras_2d1)
b1 <- ggplot(muestras_2d1_df, aes(x = V1, y = V2)) +
  geom_density_2d(fill = "skyblue", alpha = 0.7) +
  geom_density2d_filled()+
  #geom_point(data = muestras_2d_df, aes(x = V1, y = V2), color = "white", alpha = 0.03) +
  labs(title = "",
       x = "X1",
       y = "X2") +
  theme_minimal()
################################################
r_propuesta <- function(x) rmvnorm(1, mean = x, sigma = sigma_prop2)
d_propuesta <- function(x, mean) dmvnorm(x, mean = mean, sigma = sigma_prop2)

muestras_2d2 <- sample_mh_2d(d_objetivo, r_propuesta, d_propuesta, p_inicial= c(runif(1,0,1), runif(1,-0.5, 1)), n=5000)

muestras_2d2_df <- as.data.frame(muestras_2d2)
b2 <- ggplot(muestras_2d2_df, aes(x = V1, y = V2)) +
  geom_density_2d(fill = "skyblue", alpha = 0.7) +
  geom_density2d_filled()+
  #geom_point(data = muestras_2d_df, aes(x = V1, y = V2), color = "white", alpha = 0.03) +
  labs(title = "",
       x = "X1",
       y = "X2") +
  theme_minimal()
#################################################
r_propuesta <- function(x) rmvnorm(1, mean = x, sigma = sigma_prop3)
d_propuesta <- function(x, mean) dmvnorm(x, mean = mean, sigma = sigma_prop3)

muestras_2d3 <- sample_mh_2d(d_objetivo, r_propuesta, d_propuesta, p_inicial= c(runif(1,0,1), runif(1,-0.5, 1)), n=5000)

muestras_2d3_df <- as.data.frame(muestras_2d3)
b3 <- ggplot(muestras_2d3_df, aes(x = V1, y = V2)) +
  geom_density_2d(fill = "skyblue", alpha = 0.7) +
  geom_density2d_filled()+
  #geom_point(data = muestras_2d_df, aes(x = V1, y = V2), color = "white", alpha = 0.03) +
  labs(title = "",
       x = "X1",
       y = "X2") +
  theme_minimal()
```

Se obtienen muestras utilizando tres matrices de covariancias distintas para la distribución que propone los saltos. Estas son:
$$\Sigma_1=\begin{pmatrix}
   4 & 1 \\
   1 & 4
\end{pmatrix} $$
Las muestras del primer postirior resultan de la forma
```{r}
#b1
```



$$\Sigma_2=\begin{pmatrix}
   1 & 0 \\
   0 & 1
\end{pmatrix} $$
Las muestrar del segundo postirior obtenida resultan de la forma
```{r}
#b2
```

$$\Sigma_3=\begin{pmatrix}
   2 & -2 \\
   -2 & 5
\end{pmatrix} $$
Las muestrar del tercer postirior obtenida resultan de la forma
```{r, echo=FALSE}
#b3
```

Se comparan las trayectorias para las distintas muestras obtenidas, y posteriormente se analizan las funciones de autocorrelación:
```{r}
# Añadir una columna para indicar la cadena
muestras_2d1_df$cadena <- "Muestra 1"
muestras_2d2_df$cadena <- "Muestra 2"
muestras_2d3_df$cadena <- "Muestra 3"
# Armar los dataframe
muestras_2d1_df$posicion <- seq(0, 1, length.out= 5000)
muestras_2d2_df$posicion <- seq(0, 1, length.out= 5000)
muestras_2d3_df$posicion <- seq(0, 1, length.out= 5000)

# Graficar las trayectorias
ejes_limites_x <- c(-2.5, 2.5)
ejes_limites_y <- c(-1.5, 6)
m1 <- ggplot(muestras_2d1_df, aes(x = V1, y = V2,alpha = posicion)) +
  geom_point(color = "#536C8D") +
  geom_line(color = "#536C8D") +
  labs(x = "X1", y = "X2", title = "Muestra 1")+
  theme(legend.position = "none")+
  coord_cartesian(xlim = ejes_limites_x, ylim = ejes_limites_y)

m2 <- ggplot(muestras_2d2_df, aes(x = V1, y = V2,alpha = posicion)) +
  geom_point(color = "#62A07B") +
  geom_line(color = "#62A07B") +
  labs(x = "X1", y = "X2", title = "Muestra 2")+
  theme(legend.position = "none")+
  coord_cartesian(xlim = ejes_limites_x, ylim = ejes_limites_y)

m3 <- ggplot(muestras_2d3_df, aes(x = V1, y = V2,alpha = posicion)) +
  geom_point(color = "#613860") +
  geom_line(color = "#613860") +
  labs(x = "X1", y = "X2", title = "Muestra 3")+
  theme(legend.position = "none")+
  coord_cartesian(xlim = ejes_limites_x, ylim = ejes_limites_y)

grid.arrange(m1, m2, m3, ncol = 3)
```
Se observa que la trayectoria de la primera y tercera muestra parecen presentar valores en todo el recorrido de la función de Rosenbrock, mientras que la muestra obtenida con la segunda cadena se restringe a los valores centrales de dicha función.

```{r}
#Trace plot y gráfico de autocorrelación
n=5000
df1 <- as.data.frame(cbind(muestras_2d1_df$V1,muestras_2d2_df$V1,muestras_2d3_df$V1, x=1:n))
tp1=ggplot(df1, aes(x=x)) +
  geom_line(aes(y = V1), color = "#536C8D") +
  geom_line(aes(y = V2), color = "#62A07B") +
  geom_line(aes(y = V3), color = "#613860") +
  labs(x = "Numero de muestra X1", y = "Valor", title = "")

df2 <- as.data.frame(cbind(muestras_2d1_df$V2,muestras_2d2_df$V2,muestras_2d3_df$V2, x=1:n))
tp2=ggplot(df2, aes(x=x)) +
  geom_line(aes(y = V1), color = "#536C8D") +
  geom_line(aes(y = V2), color = "#62A07B") +
  geom_line(aes(y = V3), color = "#613860") +
  labs(x = "Numero de muestra X2", y = "Valor", title = "")

grid.arrange(tp1, tp2, ncol=1)

corr11 <- acf(muestras_2d1_df[,1], plot = FALSE, lag = Inf)
corr12 <- acf(muestras_2d1_df[,2], plot = FALSE, lag = Inf)
corr21 <- acf(muestras_2d2_df[,1], plot = FALSE, lag = Inf)
corr22 <- acf(muestras_2d2_df[,2], plot = FALSE, lag = Inf)
corr31 <- acf(muestras_2d3_df[,1], plot = FALSE, lag = Inf)
corr32 <- acf(muestras_2d3_df[,2], plot = FALSE, lag = Inf)

corr_x1=data.frame(lag= c(corr11$lag, corr21$lag, corr31$lag),
           acf= c(corr11$acf, corr21$acf, corr31$acf),
           cadena= rep(c("Muestra 1", "Muestra 2", "Muestra 3"), each=5000))

corr_x2=data.frame(lag= c(corr12$lag, corr22$lag, corr32$lag),
           acf= c(corr12$acf, corr22$acf, corr32$acf),
           cadena= rep(c("Muestra 1", "Muestra 2", "Muestra 3"), each=5000))

grafico_corr_x1= ggplot(corr_x1)+
  aes(x=lag,y=acf, color=cadena)+
  geom_line()+
  labs(x = "Lag", y = "Autocorrelación", title = "")+
  scale_color_manual(values= c("#536C8D","#62A07B","#613860"))+
  guides(color = guide_legend(title = "Cadenas"))

grafico_corr_x2= ggplot(corr_x2)+
  aes(x=lag,y=acf, color=cadena)+
  geom_line()+
  labs(x = "Lag", y = "Autocorrelación", title = "")+
  scale_color_manual(values= c("#536C8D","#62A07B","#613860"))+
  guides(color = guide_legend(title = "Cadenas"))

grid.arrange(grafico_corr_x1,grafico_corr_x2, ncol=1)

#Calculamos el n eff de muestras para ver que muestra resulto mejor
tabla_Neff <- rbind(round(N_eff(muestras_2d1),2), round(N_eff(muestras_2d2),2), round(N_eff(muestras_2d3),2))

colnames(tabla_Neff) <- c("$N_{eff}\\ para\\ X_1$", "$N_{eff}\\ para\\ X_2$")
rownames(tabla_Neff) <- c("Muestra 1","Muestra 2", "Muestra 3")
kable(tabla_Neff, "html", escape = FALSE)%>% 
  kable_styling(position = "center",full_width = FALSE) %>% 
  row_spec(0, bold = T, background = "#613860", color = "black")
```
A diferencia de las muestras 1 y 2, la tercera muestra presenta una mayor autocorrelación en sus observaciones, y debido a esto se puede observar que también el número efectivo de muestras disminuye en prácticamente la mitad de las observaciones.
Esto podría deberse a que la matriz de covarianza utilizada en esta cadena no acompaña la forma de la distribución a posteriori.


```{r}
# Ej 10
# Calculamos las probabilidades con la segunda muestra, la cual presenta el mayor número efectivo de muestras
probs <- numeric(3)
# 0 < x1 < 1  ;0 < x2 < 1
mu_filtradas <- muestras_2d2[muestras_2d2[,1] > 0 & muestras_2d2[,1] < 1 & muestras_2d2[,2] > 0 & muestras_2d2[,2] < 1,]
probs[1] <- nrow(mu_filtradas) / nrow(muestras_2d2)
# -1 < x1 < 0  ;0 < x2 < 1 
mu_filtradas <- muestras_2d2[muestras_2d2[,1] > -1 & muestras_2d2[,1] < 0 & muestras_2d2[,2] > 0 & muestras_2d2[,2] < 1,]
probs[2] <- nrow(mu_filtradas) / nrow(muestras_2d2)
# 1 < x1 < 2 ;2 < x2 < 3
mu_filtradas <- muestras_2d2[muestras_2d2[,1] > 1 & muestras_2d2[,1] < 2 & muestras_2d2[,2] > 2 & muestras_2d2[,2] < 3,]
probs[3] <- nrow(mu_filtradas) / nrow(muestras_2d2)
probs
```

```{r}
#Ej 11
#Método de la grilla
grilla= numeric(3)
grilla[1]=sum(df_grilla$z[df_grilla$x>0 & df_grilla$x<1 & df_grilla$Var2>0 & df_grilla$Var2<1])/ sum(df_grilla$z)
grilla[2]=sum(df_grilla$z[df_grilla$x>-1 & df_grilla$x<0 & df_grilla$Var2>0 & df_grilla$Var2<1])/ sum(df_grilla$z)
grilla[3]=sum(df_grilla$z[df_grilla$x>1 & df_grilla$x<2 & df_grilla$Var2>2 & df_grilla$Var2<3])/ sum(df_grilla$z)

data_probs= cbind(muestras=probs,grilla= grilla)

colnames(data_probs) <- c("Muestras","Grilla")
rownames(data_probs) <- c("P(0< X1 < 1,0 < X2< 1)","P(-1 < X1 < 0,0 < X2 < 1)", "P(1 < X1 < 2,2 < X2 < 3)")
kable(round(data_probs,4))%>% 
  kable_styling(position = "center",full_width = FALSE) %>% 
  row_spec(0, bold = T, background = "#613860", color = "black") 
```

Básicamente, a lo largo del trabajo se ve que Metropolis Hastings es una variante que permite sacar muestras de distribuciones de probabilidad, incluso cuando no se tiene idea de cómo es exactamente esa distribución, solo necesitamos su ley. Se probó en distribuciones unidimensionales y bidimensionales como lo son las distribuciones de Kumaraswamy y Rosenbrock. Se interactuó con diferentes formas de distribución propuesta y se observó cómo esta afecta a la eficiencia del algoritmo. 
Es interesante ver a Metropolis Hastings como una herramienta en estadística bayesiana ya que ayuda a muestrear y/o graficar distribuciones a posteriori.


