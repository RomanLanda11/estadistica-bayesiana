---
title: " Trabajo Práctico 2: Metropolis-Hastings "
author: "Gazze - Irisarri - Landa"
toc: False
toc-title: 'Contenido'
output:
  pdf_document: default
  html_document: default
execute:
  warning: False
  message: False
---
\thispagestyle{empty}

\begin{center}
  \vspace*{1cm}

  \Huge
  \textbf{Estadística Bayesiana}

  \vspace{0.5cm}
  \LARGE

  \vspace{1.5cm}

  \textbf{Alumnos:} Simón Gazze, Malena Irisarri, Román Landa\\

  \vfill

  \includegraphics[width=0.4\textwidth]{logo_universidad}

  \vspace{0.8cm}


  Rosario, Argentina

  8 de mayo de 2024
\end{center}

\newpage

```{r, echo=FALSE}
#| label = parametros
set.seed(412)
knitr::opts_chunk$set(fig.align = "center")
```


```{r message=FALSE, include=FALSE}
#| label = librerias
library(ggplot2)
library(dplyr)
library(gridExtra)
library(kableExtra)
library(mvtnorm)
library(pracma)
library(MASS)
library(rgl)
library(knitr)
library(gridGraphics)
```


# Introducción

Metropolis-Hastings es un proceso para generar muestras pseudoaleatorias de distribuciones objetivo. Se trata de un proceso iterativo que comienza en un punto inicial en el espacio de muestras, luego, propone un "salto" a un nuevo punto utilizando una distribución de probabilidad propuesta. Este salto es evaluado mediante un criterio de aceptación basado en la relación entre la densidad en el nuevo punto y en el punto actual. Si el salto es aceptado, el algoritmo se mueve al nuevo punto, sino, permanece en el punto actual. 
Este proceso se repite muchas veces para explorar y muestrear de manera eficiente. En este trabajo práctico, se explorará la aplicabilidad y el rendimiento del algoritmo para distintas distribuciones objetivo y propuestas.

# Objetivo

Se propone explorar y comprender el funcionamiento y la utilidad del algoritmo de Metropolis-Hastings en diferentes situaciones. Se abordarán ejercicios que van desde la generación de muestras unidimensionales, hasta la aplicación del algoritmo en problemas más complejos como la generación de muestras de una distribución normal bivariada y la estimación de la distribución a posteriori de una funciones de densidad complejas. 
A través de estos ejercicios, se buscará evaluar el desempeño del algoritmo y comprender sus ventajas y limitaciones en la generación de muestras de distribuciones de probabilidad.

\newpage

# Metropolis Hastings en 1D

Para entrar en calor, se comienza declarando una función que implemente el algoritmo de Metropolis-Hastings para tomar muestras de una distribución de probabilidad unidimensional. El algoritmo permite elegir entre un punto de inicio arbitrario o al azar y utiliza la distribución propuesta de transición arbitraria (por defecto, se utiliza una distribución normal estándar).

##  Algoritmo MH en 1 D

```{r}
d_propuesta_def <- function(x, mean){
  dnorm(x, mean, sd= 1)
}

sample_mh <- function (d_objetivo, r_propuesta, d_propuesta = d_propuesta_def, p_inicial, n){
  stopifnot(n>0)
  salto = 0
  muestras <- numeric(n)
  muestras[1]<-p_inicial
  for (i in 2:n) {
    p_actual <- muestras[i-1]
    p_nuevo <- r_propuesta(p_actual)
    f_actual <- d_objetivo(p_actual)
    f_nuevo <- d_objetivo(p_nuevo)
    
    q_actual <- d_propuesta(p_actual,mean=p_nuevo)
    q_nuevo <- d_propuesta(p_nuevo,mean=p_actual)
    
    alpha <- min(1, (f_nuevo/f_actual)*(q_actual/q_nuevo))
    
    aceptar <- rbinom(1,1, alpha)
    if (aceptar){
      muestras[i] <- p_nuevo
      salto = salto + 1  
    } else {
      muestras[i] <- p_actual
    }
    
  }
  tasa_acepta = (salto/n)*100
  return ( muestras )
}
```

\newpage

# Distribución de Kumaraswamy

La distribución de Kumaraswamy es una distribución de probabilidad continua que se utiliza para modelar variables aleatorias con soporte en el intervalo $(0;1)$.

##  Conociendo a Jagdish Kumaraswamy

Se grafica la función de densidad de la distribución de Kumaraswamy para 5 combinaciones de los parámetros $a$ y $b$ con el objetivo de familiarizarse con su comportamiento.

```{r}
d_kumaraswamy <- function(x, a ,b){
  if(x<0 || x>1){
    f=0
  }else{
    f <- a*b*(x^(a-1))*(1-x^a)^(b-1)
  }
  return(f)
}
```


```{r, echo=FALSE}
x<-seq(0,1, length.out= 200)
d_kumaraswamy.5.5 <- numeric(length(x))
d_kumaraswamy51<- numeric(length(x))
d_kumaraswamy13 <-numeric(length(x))
d_kumaraswamy22 <- numeric(length(x))
d_kumaraswamy25 <- numeric(length(x))
for (i in 1:length(x)) {
  d_kumaraswamy.5.5[i] <- d_kumaraswamy(x[i], 0.5, 0.5)
  d_kumaraswamy51[i] <- d_kumaraswamy(x[i], 5, 1)
  d_kumaraswamy13[i] <- d_kumaraswamy(x[i], 1, 3)
  d_kumaraswamy22[i] <- d_kumaraswamy(x[i], 2, 2)
  d_kumaraswamy25[i] <- d_kumaraswamy(x[i], 2, 5)
}


df <- data.frame(x,
                 d_kumaraswamy.5.5,
                 d_kumaraswamy51,
                 d_kumaraswamy13,
                 d_kumaraswamy22,
                 d_kumaraswamy25)
ggplot(df, aes(x = x)) +
  # Agregar las líneas para cada distribución
  geom_line(aes(y = d_kumaraswamy.5.5, color = "d_kumaraswamy.5.5")) +
  geom_line(aes(y = d_kumaraswamy51, color = "d_kumaraswamy51")) +
  geom_line(aes(y = d_kumaraswamy13, color = "d_kumaraswamy13")) +
  geom_line(aes(y = d_kumaraswamy22, color = "d_kumaraswamy22")) +
  geom_line(aes(y = d_kumaraswamy25, color = "d_kumaraswamy25")) +
  # Personalizar aspectos del gráfico
  labs(title = "",
       x = "X",
       y = "Densidad") +
  scale_color_manual(values = c("d_kumaraswamy.5.5" = "blue",
                                 "d_kumaraswamy51" = "green",
                                 "d_kumaraswamy13" = "red",
                                 "d_kumaraswamy22" = "cyan",
                                 "d_kumaraswamy25" = "purple"),
                     labels = c("d_kumaraswamy.5.5" = "a = 0.5, b = 0.5",
                                "d_kumaraswamy51" = "a = 5, b = 1",
                                "d_kumaraswamy13" = "a = 1, b = 3",
                                "d_kumaraswamy22" = "a = 2, b = 2",
                                "d_kumaraswamy25" = "a = 2, b = 5")) +
  guides(color=guide_legend(title = "Parámetros"))+
  theme_minimal()
```
La distribución de Kumaraswamy resulta muy útil a la hora de realizar estudios bayesianos, ya que presenta una gran flexibilidad y puede modelar una amplia gama de formas de distribuciones de probabilidad.
Ademas presenta una función de densidad de probabilidad simple y computacionalmente eficiente, lo que facilita su uso en cálculos bayesianos.
Por último, vale destacar que la restricción al intervalo $(0;1)$ resulta útil porque se adapta naturalmente al modelado de proporciones y probabilidades.

##  M-H para Jagdish

Se utiliza la función construida al comienzo del informe para obtener 5000 muestras de una distribución de Kumaraswamy con parámetros $a=6 , b=2$ y una distribución de propuesta beta.

```{r}
get_beta_pars <- function(mu, kappa){
  alpha <- mu *kappa
  beta <- (1-mu)*kappa
  return(list(alpha = alpha, beta = beta))
}

d_objetivo <- function(x, a=6 ,b=2){
  if(x<0 || x>1){
    f=0
  }else{
    f <- a*b*(x^(a-1))*(1-x^a)^(b-1)
  }
  return(f)
}
```

Se observan las cadenas obtenidas utilizando tres grados de concentración distintos $kappas = 1$, $25$ y $50$

```{r, echo=FALSE}
kappas <- c(1,25,50)
n=5000
muestras <- matrix(nrow = n,ncol = length(kappas))
for (i in 1:3) {
  p_inicial = runif(1) 
  r_propuesta <- function(x) {
    kappa <- kappas[i]
    pars <- get_beta_pars(x, kappa)
    rbeta(1, pars$alpha, pars$beta)
  }
  d_propuesta <- function(x, mean) {
    kappa <- kappas[i]
    pars <- get_beta_pars(mean, kappa)
    dbeta(x, pars$alpha, pars$beta)
  }
  p_inicial <- runif(1)
  
  muestras[,i] <- sample_mh(d_objetivo, r_propuesta, d_propuesta, p_inicial= p_inicial, n=n)
}


df <- as.data.frame(cbind(muestras, x=1:n))
ggplot(df, aes(x = x)) +
  geom_line(aes(y = V1), color = "#536C8D") +
  geom_line(aes(y = V2), color = "#62A07B") +
  geom_line(aes(y = V3), color = "#613860") +
  labs(x = "Muestras", y = "Valor", title = "")+
  theme_minimal()

```

```{r, echo=FALSE}
g1 <- ggplot(df, aes(x = V1)) +
  geom_histogram(color = "black", fill = "#536C8D", bins = 30) +
  labs(x = "Muestra 1", y = "Frecuencia", title = "")+
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 500))+
  theme_minimal()

# Graficar histograma para V2
g2 <- ggplot(df, aes(x = V2)) +
  geom_histogram(color = "black", fill = "#62A07B", bins = 30) +
  labs(x = "Muestra 2", y = "Frecuencia", title = "")+
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 500))+
  theme_minimal()

# Graficar histograma para V3
g3 <- ggplot(df, aes(x = V3)) +
  geom_histogram(color = "black", fill = "#613860", bins = 30) +
  labs(x = "Muestra 3", y = "Frecuencia", title = "")+
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 500))+
  theme_minimal()

# Mostrar los histogramas en una sola columna
grid.arrange(g1, g2, g3, ncol = 1)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
calcular_w <- function(x){
  mean(apply(x, 2, var))
}
calcular_b <- function(x){
  media_chain <- apply(x, 2, mean)
  var(media_chain) * nrow(x)
}
R_hat <- function(x){
  S <- nrow(x)
  M <- ncol(x)
  W <- calcular_w(x)
  B <- calcular_b(x)
  numerador <- (S-1)/S * W + (1 / S) * B
  sqrt(numerador / W)
}
```


```{r message=FALSE, warning=FALSE, include=FALSE}
R_hat(muestras) 
```


```{r message=FALSE, warning=FALSE, include=FALSE}
corr <- acf(muestras[,3], plot = FALSE, lag = Inf)
N_eff <- function(x){
  n_eff <- numeric(ncol(x))
  for (i in 1:ncol(x)) {
    S <- nrow(x)
    autocorr <- acf(x[,i], plot = FALSE, lag = Inf)$acf
    limite <- which(autocorr<0.05)[1]
    
    denominador <- 1 + 2 * sum(autocorr[2:limite])
    n_eff[i] <- S / denominador  
  }
  return(n_eff)
}
```


```{r message=FALSE, warning=FALSE, include=FALSE}
N_eff(muestras)
```


```{r message=FALSE, warning=FALSE, include=FALSE, echo=FALSE}
data_corr = data.frame(x=corr$lag, y=corr$acf)
ggplot(data_corr)+
  aes(x=x,y=y)+
  geom_line()+
  geom_hline(yintercept = 0, linetype = "dashed", color = "#613860")+
  labs(x = "Lag", y = "Autocorrelación", title = "")

```

Lo primero que se puede mencionar es la diferencia en las tasas de aceptación, las cuales fueron $21.72$ %, $77.9$% y $81.66$% y los números efectivos de muestras $522.36$, $345.08$ y $133.14$ respectivamente. En cuanto a los recorridos de las muestras, se observa aleatoridad o un patrón de *ruido blanco*. Finalmente observamos el $\hat{R}$ el cual resultó ser $1.003404$ lo que indica converngencia entren cadenas.
Tambien, se presentan histogramas de frecuencias observadas para las distintas muestras y se observan comportamientos similares para las tres concentraciones elegidas.

##  Explorando cadenas

Se presenta una tabla con la media de la distribución y los percentiles 5 y 95 de cada una de las cadenas obtenidas.

```{r, echo=FALSE}
medias <- c(mean(muestras[,1]),mean(muestras[,2]),mean(muestras[,3]))
percentiles<- rbind(quantile(muestras[,1], c(0.05, 0.95)),
                    quantile(muestras[,2], c(0.05, 0.95)),
                    quantile(muestras[,3], c(0.05, 0.95)))

tabla <-cbind(medias, percentiles)
colnames(tabla) <- c("Medias","Percentil 5", "Percentil 95")
rownames(tabla) <- c("Kappa = 1","Kappa = 25", "Kappa = 50")

tabla_redondeada <- round(tabla, 3)
kable(tabla_redondeada) %>%
  kable_styling(position = "center", full_width = FALSE) %>%
  row_spec(0, bold = T, background = "#8b7991", color = "black")
```
\newpage

Se replica la misma tabla para el $logit(x)$ de las muestras. 

```{r, echo=FALSE}
muestras_logit <- log(muestras/(1-muestras))
medias_logit <- c(mean(muestras_logit[,1]),mean(muestras_logit[,2]),mean(muestras_logit[,3]))
percentiles_logit <- rbind(quantile(muestras_logit[,1], c(0.05, 0.95)),
                    quantile(muestras_logit[,2], c(0.05, 0.95)),
                    quantile(muestras_logit[,3], c(0.05, 0.95)))

tabla_logit <-cbind(medias_logit, percentiles_logit)
colnames(tabla_logit) <- c("Medias","Percentil 5", "Percentil 95")
rownames(tabla_logit) <- c("Kappa = 1","Kappa = 25", "Kappa = 50")
tabla_redondeada <- round(tabla_logit,3)
kable(tabla_redondeada)%>%
  kable_styling(position = "center",full_width = FALSE) %>%
  row_spec(0, bold = T, background = "#8b7991", color = "black")
```
\newpage

# Metropolis-Hastings en 2D

Para extender el algoritmo de Metropolis-Hastings se generaliza la función permitiendole implementar el proceso para más de una dimensión. 
Se plantea una probabilidad de salto normal bivariada de matriz de covarianza variable y se otorga flexibilidad al algoritmo haciendo que reciba como argumento la matriz de covarianza de la probabilidad de transición.

```{r}
sample_mh_2d <- function (d_objetivo, r_propuesta, d_propuesta, p_inicial, n, sigma_prop){
  stopifnot(n>0)
  salto = 0
  muestras <- matrix(nrow = n, ncol = length(p_inicial))
  muestras[1,]<-p_inicial
  for (i in 2:n) {
    p_actual <- muestras[i-1,]
    p_nuevo <- r_propuesta(p_actual)
    f_actual <- d_objetivo(p_actual)
    f_nuevo <- d_objetivo(p_nuevo)
    
    q_actual <- d_propuesta(p_actual,mean=p_nuevo)
    q_nuevo <- d_propuesta(p_nuevo,mean=p_actual)
    
    alpha <- min(1, (f_nuevo/f_actual)*(q_actual/q_nuevo))
    
    aceptar <- rbinom(1,1, alpha)
    if (aceptar){
      muestras[i,] <- p_nuevo
      salto = salto + 1
    } else {
      muestras[i,] <- p_actual
    }
    
  }
  tasa_acepta <- (salto/n)*100
  #print(tasa_acepta)
  return ( muestras ) 
}
```

## Conociendo a MH en 2D

Para comenzar a explorar el comportamiento del algoritmo en dos dimensiones se obtendrán muestras de una distribución normal bivariada con media $$ \mu = \begin{pmatrix}
   0.4 \\
   0.75
\end{pmatrix} $$
 y matriz de covarianza $$\Sigma=\begin{pmatrix}
   0.35 & 0.4 \\
   0.4 & 2.4
\end{pmatrix} $$

Se comienza trazando la densidad de la distribución objetivo para entenderla mejor y seleccionar un punto inicial que esté dentro de su rango. 

```{r warning=FALSE, echo=FALSE}
mu_obj <- c(0.4, 0.75)
sigma_obj <- matrix(c(1.35, 0.4, 0.4, 2.4), ncol = 2)

sigma_prop <- diag(2) 

d_objetivo <- function(x) dmvnorm(x, mean = mu_obj, sigma = sigma_obj)
r_propuesta <- function(x) rmvnorm(1, mean = x, sigma = sigma_prop)
d_propuesta <- function(x, mean) dmvnorm(x, mean = mean, sigma = sigma_prop)

df_grilla <- expand.grid(x = seq(-3, 3, length.out = 200), seq(-3, 4, length.out = 200))
df_grilla$z <- d_objetivo(as.matrix(df_grilla))

ggplot(df_grilla)+
  aes(x = x, y = Var2, z = z)+
  stat_contour()+
  labs(title = "",
       x = "X1",
       y = "X2")
```


```{r warning=FALSE, echo=FALSE}
# Generar datos para graficar
n <- 50
x <- seq(-3, 3, length.out = n)
y <- seq(-3, 3, length.out = n)
z <- matrix(0, nrow = n, ncol = n)

# Calcular la densidad de probabilidad para cada combinación de x e y
for (i in 1:n) {
  for (j in 1:n) {
    z[i, j] <- dmvnorm(c(x[i], y[j]), mean = mu_obj, sigma = sigma_obj)
  }
}

# Graficar
graf3d <- persp(x, y, z, theta = 30, phi = 20, expand = 0.5, col = "#8b7991",
      xlab = "X", ylab = "Y", zlab = "Z", ticktype = "detailed", sub = "Función de densidad de probabilidad")

```


Luego se utiliza la función **sample_mh_2d** para obtener cinco mil muestras de esta distribución y se grafican.

```{r warning=FALSE, echo=FALSE}
set.seed(412)
muestras_2d <- sample_mh_2d(d_objetivo, r_propuesta, d_propuesta, p_inicial=c(runif(1,-2,3),runif(1,-2,3)), n=5000)

muestras_2d_df <- as.data.frame(muestras_2d)
ggplot(muestras_2d_df, aes(x = V1, y = V2)) +
  geom_density_2d(fill = "skyblue", alpha = 0.7) +
  geom_density2d_filled()+
  geom_point(data = muestras_2d_df, aes(x = V1, y = V2), color = "white", alpha = 0.03) +
  labs(title = "",
       x = "X1",
       y = "X2") +
  stat_contour(aes(x = x, y = Var2, z = z), data = df_grilla, color = "white")+
  theme_minimal()
```
Se presenta a continuación un trace plot para ver la correlación entre las muestras.
```{r}
plot_trace <- function(x, y){
  n <-length(x)
  df <- data.frame(x = rep(1:n, 2), y = c(x, y), dimension = rep(1:2, each = n))
  ggplot(df)+
    geom_line(aes(x = x, y = y), color = "#536C8D")+
    facet_grid(rows = vars(dimension))+
    labs(x = "Muestra", y = "Valor")
}  
plot_trace(muestras_2d_df$V1, muestras_2d_df$V2)+
  theme_minimal()
```


```{r include=FALSE}
#Número efectivo de muestras
N_eff(muestras_2d)
```
Se observa nuevamente un patrón de *ruido blanco* en los Trace Plot, lo cual indica una muestra poco correlacionada a través de las observaciones. Mientras que el número efectivo de muestras para $X_1$ es igual a $481.98$ y para $X_2$ resulta $363.31$, lo cual indica que las cinco mil muestras correlacionadas equivalen a cuatrocientas ochenta y dos y trecientas secenta y tres muestras independientes, respectivamente. 

A fines de comparar la calidad de las muestras obtenidas, se calculan y comparan algunas probabilidades mediante distintos métodos.

```{r, echo=FALSE}
#| label = probs
probs <- numeric(3)
# x1 > 1 ; x2 < 0
mu_filtradas <- muestras_2d[muestras_2d[,1] > 1 & muestras_2d[,2] < 0,]
probs[1] <- nrow(mu_filtradas) / nrow(muestras_2d)
# x1 > 1 ; x2 > 2 
mu_filtradas <- muestras_2d[muestras_2d[,1] > 1 & muestras_2d[,2] > 2,]
probs[2] <- nrow(mu_filtradas) / nrow(muestras_2d)
# x1 > .4 ; x2 > .75
mu_filtradas <- muestras_2d[muestras_2d[,1] > .4 & muestras_2d[,2] > .75,]
probs[3] <- nrow(mu_filtradas) / nrow(muestras_2d)
```

```{r, echo=FALSE}
# Monte Carlo generamos aleatorios y hace sumatoria
# Genera un gran número de muestras bivariadas
n <- 5000
muestras_2d_mc <- rmvnorm(n, mean = mu_obj, sigma = sigma_obj)
p_exacta <- numeric(3)
p_mc <- numeric(3)
# x1 > 1 ; x2 < 0
mu_filtradas <- muestras_2d_mc[muestras_2d_mc[,1] > 1 & muestras_2d_mc[,2] < 0,]
p_mc[1] <- nrow(mu_filtradas) / nrow(muestras_2d_mc)
p_exacta[1] <- pmvnorm(lower = c(1, -Inf), upper = c(Inf,0), mean = mu_obj, sigma=sigma_obj)
# x1 > 1 ; x2 > 2 
mu_filtradas <- muestras_2d_mc[muestras_2d_mc[,1] > 1 & muestras_2d_mc[,2] > 2,]
p_mc[2] <- nrow(mu_filtradas) / nrow(muestras_2d_mc)
p_exacta[2] <- pmvnorm(lower = c(1, 2), upper = c(Inf,Inf), mean = mu_obj, sigma=sigma_obj)
# x1 > .4 ; x2 > .75
mu_filtradas <- muestras_2d_mc[muestras_2d_mc[,1] > .4 & muestras_2d_mc[,2] > .75,]
p_mc[3] <- nrow(mu_filtradas) / nrow(muestras_2d_mc)
p_exacta[3] <- pmvnorm(lower = c(.4, .75), upper = c(Inf,Inf), mean = mu_obj, sigma=sigma_obj)
```


```{r, echo=FALSE}
#Método de la grilla
grilla= numeric(3)
grilla[1]=sum(df_grilla$z[df_grilla$x>1 & df_grilla$Var2<0])/ sum(df_grilla$z)
grilla[2]=sum(df_grilla$z[df_grilla$x>1 & df_grilla$Var2>2])/ sum(df_grilla$z)
grilla[3]=sum(df_grilla$z[df_grilla$x>0.4 & df_grilla$Var2>0.75])/ sum(df_grilla$z)

data_probs= cbind(muestras=probs, mc= p_mc, grilla= grilla, exacto= p_exacta)

colnames(data_probs) <- c("Muestras","Montecarlo", "Grilla","Prob. Exacta")
rownames(data_probs) <- c("P(X 1> 1, X2 < 0)","P(X1 > 1, X2 > 2)", "P(X1 > 0.4, X2 > 0.75)")
tabla_redondeada <- round(data_probs,4)
kable(tabla_redondeada)%>%
  kable_styling(position = "center",full_width = FALSE) %>%
  row_spec(0, bold = T, background = "#8b7991", color = "black")

```
Como se observa en la tabla, si bien las probabilidades obtenidas mediante el metodo de Monte Carlo son las que más se aproximan a las reales, con el método de Metropolis Hasting se obtienen muy buenas aproximaciones. Vale aclarar que en situaciones más complejas, donde se dificulta el cálculo de las probabilidades exactas, la aplicación de MH toma mayor importancia.

\newpage

# Función de Rosenbrock

La función de Rosenbrock, comunmente conocida como la “banana de Rosenbrock”, es una función matemática utilizada frecuentemente como un problema de optimización y prueba para algoritmos de optimización numérica. También es muy conocida en el campo de la estadística bayesiana, ya que en ciertos escenarios, la densidad del posterior toma una forma que definitivamente se asemeja a la banana de Howard Rosenbrock.

Un ejemplo de esta se presenta a continuación:
$$p^*(x_1, x_2 \mid a, b) = \exp \left\{-\left[(a - x_1) ^ 2 + b(x_2 - x_1^2) ^ 2\right] \right\}$$

## Conociendo a Howard Rosenbrock

El objetivo en este tramo del trabajo es obtener muestras de dicha función utilizando el algoritmo de Metropolis Hasting, para esto, se comienza graficandola para conocer su recorrido y asi poder seleccionar un punto inicial.

```{r echo=FALSE, message=FALSE, warning=FALSE}
set.seed(412)
a <- 0.5
b <- 5
sigma_prop1 <- matrix(c(4,1,1,4), ncol = 2)
sigma_prop2 <- diag(2)*1
sigma_prop3 <- matrix(c(2,-2,-2,5), ncol = 2)

d_objetivo <- function(x) exp(-((a-x[1])^2+b*(x[2]-(x[1])^2)^2))
r_propuesta <- function(x) rmvnorm(1, mean = x, sigma = sigma_prop1)
d_propuesta <- function(x, mean) dmvnorm(x, mean = mean, sigma = sigma_prop1)

df_grilla <- expand.grid(x = seq(-2, 2, length.out = 200), seq(-1, 5, length.out = 200))
for(i in 1:40000){ 
df_grilla$z[i] <- d_objetivo(c(df_grilla$x[i], df_grilla$Var2[i]))
}

ggplot(df_grilla)+
 aes(x = x, y = Var2, z = z)+
 stat_contour()+
 labs(title = "",
      x = "X1",
      y = "X2")
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
d_objetivo2 <- function(x, y) exp(-((0.5-x)^2+5*(y-(x)^2)^2))
# Generar datos para graficar
n <- 100
x <- seq(-2, 2, length.out = n)
y <- seq(-1, 3, length.out = n)
z <- outer(x, y, d_objetivo2)

# Graficar
persp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = "#8b7991",
      xlab = "X", ylab = "Y", zlab = "Z", ticktype = "detailed")
title(sub = "Función de Rosenbrock")

```

```{r echo=FALSE, message=FALSE, warning=FALSE}
muestras_2d1 <- sample_mh_2d(d_objetivo, r_propuesta, d_propuesta, p_inicial= c(runif(1,0,1), runif(1,-0.5, 1)), n=5000)
muestras_2d1_df <- as.data.frame(muestras_2d1)
b1 <- ggplot(muestras_2d1_df, aes(x = V1, y = V2)) +
  geom_density_2d(fill = "skyblue", alpha = 0.7) +
  stat_contour(aes(x = x, y = Var2, z = z), data = df_grilla, color = "white")+
  geom_density2d_filled()+
  labs(title = "",
       x = "X1",
       y = "X2") +
  theme_minimal()
################################################
r_propuesta <- function(x) rmvnorm(1, mean = x, sigma = sigma_prop2)
d_propuesta <- function(x, mean) dmvnorm(x, mean = mean, sigma = sigma_prop2)

muestras_2d2 <- sample_mh_2d(d_objetivo, r_propuesta, d_propuesta, p_inicial= c(runif(1,0,1), runif(1,-0.5, 1)), n=5000)

muestras_2d2_df <- as.data.frame(muestras_2d2)
b2 <- ggplot(muestras_2d2_df, aes(x = V1, y = V2)) +
  geom_density_2d(fill = "skyblue", alpha = 0.7) +
  geom_density2d_filled()+
  stat_contour(aes(x = x, y = Var2, z = z), data = df_grilla, color = "white")+
  labs(title = "",
       x = "X1",
       y = "X2") +
  theme_minimal()
#################################################
r_propuesta <- function(x) rmvnorm(1, mean = x, sigma = sigma_prop3)
d_propuesta <- function(x, mean) dmvnorm(x, mean = mean, sigma = sigma_prop3)

muestras_2d3 <- sample_mh_2d(d_objetivo, r_propuesta, d_propuesta, p_inicial= c(runif(1,0,1), runif(1,-0.5, 1)), n=5000)

muestras_2d3_df <- as.data.frame(muestras_2d3)
b3 <- ggplot(muestras_2d3_df, aes(x = V1, y = V2)) +
  geom_density_2d(fill = "skyblue", alpha = 0.7) +
  geom_density2d_filled()+
  stat_contour(aes(x = x, y = Var2, z = z), data = df_grilla, color = "white")+
  labs(title = "",
       x = "X1",
       y = "X2") +
  theme_minimal()
```

Se obtienen muestras utilizando tres matrices de covariancias distintas para la distribución que propone los saltos. Estas son:
$$\Sigma_1=\begin{pmatrix}
   4 & 1 \\
   1 & 4
\end{pmatrix} $$
Las muestras del primer *posterior* resultan de la forma
```{r, echo=FALSE}
b1
```



$$\Sigma_2=\begin{pmatrix}
   1 & 0 \\
   0 & 1
\end{pmatrix} $$
Las muestrar del segundo *posterior* obtenida resultan de la forma
```{r, echo=FALSE}
b2
```

$$\Sigma_3=\begin{pmatrix}
   2 & -2 \\
   -2 & 5
\end{pmatrix} $$
Las muestrar del tercer *posterior* obtenida resultan de la forma
```{r, echo=FALSE}
b3
```

Se comparan las trayectorias para las distintas muestras obtenidas, y posteriormente se analizan las funciones de autocorrelación:
```{r, echo=FALSE, ,  out.width = "100%"}
# Añadir una columna para indicar la cadena
muestras_2d1_df$cadena <- "Muestra 1"
muestras_2d2_df$cadena <- "Muestra 2"
muestras_2d3_df$cadena <- "Muestra 3"
# Armar los dataframe
muestras_2d1_df$posicion <- seq(0, 1, length.out= 5000)
muestras_2d2_df$posicion <- seq(0, 1, length.out= 5000)
muestras_2d3_df$posicion <- seq(0, 1, length.out= 5000)

# Graficar las trayectorias
ejes_limites_x <- c(-2.5, 2.5)
ejes_limites_y <- c(-1.5, 6)
m1 <- ggplot(muestras_2d1_df, aes(x = V1, y = V2,alpha = posicion)) +
  geom_point(color = "#536C8D") +
  geom_line(color = "#536C8D") +
  labs(x = "X1", y = "X2", title = "Muestra 1")+
  theme(legend.position = "none")+
  coord_cartesian(xlim = ejes_limites_x, ylim = ejes_limites_y)+
  theme_minimal()

m2 <- ggplot(muestras_2d2_df, aes(x = V1, y = V2,alpha = posicion)) +
  geom_point(color = "#62A07B") +
  geom_line(color = "#62A07B") +
  labs(x = "X1", y = "X2", title = "Muestra 2")+
  theme(legend.position = "none")+
  coord_cartesian(xlim = ejes_limites_x, ylim = ejes_limites_y)+
  theme_minimal()

m3 <- ggplot(muestras_2d3_df, aes(x = V1, y = V2,alpha = posicion)) +
  geom_point(color = "#613860") +
  geom_line(color = "#613860") +
  labs(x = "X1", y = "X2", title = "Muestra 3")+
  theme(legend.position = "none")+
  coord_cartesian(xlim = ejes_limites_x, ylim = ejes_limites_y)+
  theme_minimal()

grid.arrange(m1, m2, m3, ncol = 3)
```

Se observa que la trayectoria de la primera y tercera muestra parecen presentar valores en todo el recorrido de la función de Rosenbrock, mientras que la muestra obtenida con la segunda cadena se restringe a los valores centrales de dicha función.

A continuación se presentan los *trace plot*, gráficos de autocorrelación y números efectivos de muestras a fin de comparar cómo se comporta el método según la matriz de covariancia propuesta que se utilice.

```{r, echo=FALSE}
#Trace plot y gráfico de autocorrelación
n=5000
df1 <- as.data.frame(cbind(muestras_2d1_df$V1,muestras_2d2_df$V1,muestras_2d3_df$V1, x=1:n))
tp1=ggplot(df1, aes(x=x)) +
  geom_line(aes(y = V1), color = "#536C8D") +
  geom_line(aes(y = V2), color = "#62A07B") +
  geom_line(aes(y = V3), color = "#613860") +
  labs(x = "Muestra X1", y = "Valor", title = "")

df2 <- as.data.frame(cbind(muestras_2d1_df$V2,muestras_2d2_df$V2,muestras_2d3_df$V2, x=1:n))
tp2=ggplot(df2, aes(x=x)) +
  geom_line(aes(y = V1), color = "#536C8D") +
  geom_line(aes(y = V2), color = "#62A07B") +
  geom_line(aes(y = V3), color = "#613860") +
  labs(x = "Muestra X2", y = "Valor", title = "")

grid.arrange(tp1, tp2, ncol=1)

corr11 <- acf(muestras_2d1_df[,1], plot = FALSE, lag = Inf)
corr12 <- acf(muestras_2d1_df[,2], plot = FALSE, lag = Inf)
corr21 <- acf(muestras_2d2_df[,1], plot = FALSE, lag = Inf)
corr22 <- acf(muestras_2d2_df[,2], plot = FALSE, lag = Inf)
corr31 <- acf(muestras_2d3_df[,1], plot = FALSE, lag = Inf)
corr32 <- acf(muestras_2d3_df[,2], plot = FALSE, lag = Inf)

corr_x1=data.frame(lag= c(corr11$lag, corr21$lag, corr31$lag),
           acf= c(corr11$acf, corr21$acf, corr31$acf),
           cadena= rep(c("Muestra 1", "Muestra 2", "Muestra 3"), each=5000))

corr_x2=data.frame(lag= c(corr12$lag, corr22$lag, corr32$lag),
           acf= c(corr12$acf, corr22$acf, corr32$acf),
           cadena= rep(c("Muestra 1", "Muestra 2", "Muestra 3"), each=5000))

grafico_corr_x1= ggplot(corr_x1)+
  aes(x=lag,y=acf, color=cadena)+
  geom_line()+
  labs(x = "Lag", y = "Autocorrelación", title = "")+
  scale_color_manual(values= c("#536C8D","#62A07B","#613860"))+
  guides(color = guide_legend(title = "Cadenas"))

grafico_corr_x2= ggplot(corr_x2)+
  aes(x=lag,y=acf, color=cadena)+
  geom_line()+
  labs(x = "Lag", y = "Autocorrelación", title = "")+
  scale_color_manual(values= c("#536C8D","#62A07B","#613860"))+
  guides(color = guide_legend(title = "Cadenas"))

grid.arrange(grafico_corr_x1,grafico_corr_x2, ncol=1)

#Calculamos el n eff de muestras para ver que muestra resulto mejor
tabla_Neff <- rbind(round(N_eff(muestras_2d1),2), round(N_eff(muestras_2d2),2), round(N_eff(muestras_2d3),2))

colnames(tabla_Neff) <- c("N° efectivo de muestras para X1", "N° efectivo de muestras para X2")
rownames(tabla_Neff) <- c("Muestra 1","Muestra 2", "Muestra 3")
kable(tabla_Neff)%>%
  kable_styling(position = "center",full_width = FALSE) %>%
  row_spec(0, bold = T, background = "#8b7991", color = "black")
```
  
A diferencia de las muestras 1 y 2, la tercera muestra presenta una mayor autocorrelación en sus observaciones, y debido a esto se puede observar que también el número efectivo de muestras disminuye prácticamente en la mitad de las observaciones.
Además la probabilidad de aceptación para la primer cadena es $8.24$, es decir que el algoritmo acepta el $8.24$% de los pasos propuestos, significando que queda muchas veces con el mismo valor de la muestra. Similar ocurre con la cadena 3 que acepta el $10.28$ de las veces, mientras que el mayor porcentaje de aceptación es de la cadena 3 con un $19.22$%.
Todas estas interpretaciones podrían deberse a que las matrices de covarianzas utilizadas en las cadenas 1 y 3 no acompañan la forma de la distribución a posteriori y son muy amplias, mientras que en la 2 se concentra más en los valores centrales de la distribución y sigue más su forma.

Nuevamente, con el motivo de analizar la calidad de las muestras obtenidas, se calculan y comparan algunas probabilidades utilizando las muestras, el método de la grilla y el método de Monte Carlo. Dichas probabilidades se calculan con las muestras de la cadena 2, la cual presenta el mayor número efectivo de muestras.

```{r, echo=FALSE}

probs <- numeric(3)
# 0 < x1 < 1  ;0 < x2 < 1
mu_filtradas <- muestras_2d2[muestras_2d2[,1] > 0 & muestras_2d2[,1] < 1 & muestras_2d2[,2] > 0 & muestras_2d2[,2] < 1,]
probs[1] <- nrow(mu_filtradas) / nrow(muestras_2d2)
# -1 < x1 < 0  ;0 < x2 < 1 
mu_filtradas <- muestras_2d2[muestras_2d2[,1] > -1 & muestras_2d2[,1] < 0 & muestras_2d2[,2] > 0 & muestras_2d2[,2] < 1,]
probs[2] <- nrow(mu_filtradas) / nrow(muestras_2d2)
# 1 < x1 < 2 ;2 < x2 < 3
mu_filtradas <- muestras_2d2[muestras_2d2[,1] > 1 & muestras_2d2[,1] < 2 & muestras_2d2[,2] > 2 & muestras_2d2[,2] < 3,]
probs[3] <- nrow(mu_filtradas) / nrow(muestras_2d2)
```

```{r, echo=FALSE}
#Ej 11
#Método de la grilla
grilla= numeric(3)
grilla[1]=sum(df_grilla$z[df_grilla$x>0 & df_grilla$x<1 & df_grilla$Var2>0 & df_grilla$Var2<1])/ sum(df_grilla$z)
grilla[2]=sum(df_grilla$z[df_grilla$x>-1 & df_grilla$x<0 & df_grilla$Var2>0 & df_grilla$Var2<1])/ sum(df_grilla$z)
grilla[3]=sum(df_grilla$z[df_grilla$x>1 & df_grilla$x<2 & df_grilla$Var2>2 & df_grilla$Var2<3])/ sum(df_grilla$z)
# Monte Carlo
monte_carlo <- function(n = 5000, d_objetivo=d_objetivo2, x_min, x_max, y_min, y_max) {
  x <- runif(n, x_min, x_max)
  y <- runif(n, y_min, y_max)
  int_f <- d_objetivo2(x, y)
  result_f <- mean(int_f)
  return(result_f)
}
prob_mc <- numeric(3)
prob_mc[1] <- monte_carlo(x_min=0, x_max=1, y_min=0, y_max=1)
prob_mc[2] <- monte_carlo(x_min=-1, x_max=0, y_min=0, y_max=1)
prob_mc[3] <- monte_carlo(x_min=1, x_max=2, y_min=2, y_max=3)

# Met. Exacto
prob_ext <- numeric(3)
prob_ext[1] <- integral2(d_objetivo2, 0,1,0,1)$Q
prob_ext[2] <- integral2(d_objetivo2, -1,0,0,1)$Q
prob_ext[3] <- integral2(d_objetivo2, 1,2,2,3)$Q
```


```{r, echo=FALSE}
data_probs= cbind(muestras=probs,
                  monte_carlo=prob_mc,
                  grilla= grilla,
                  exacto=prob_ext)

colnames(data_probs) <- c("Muestras", "Montecarlo", "Grilla", "Prob. Exacta")
rownames(data_probs) <- c("P(0 < X1 < 1 ; 0 < X2 < 1)","P(-1 < X1 < 0 ; 0 < X2 < 1)", "P(1 < X1 < 2 ; 2 < X2 < 3)")
kable(round(data_probs,4))%>%
  kable_styling(position = "center",full_width = FALSE) %>%
  row_spec(0, bold = T, background = "#8b7991", color = "black")
```
Estos resultados exponen una debilidad en el algoritmo de Metropolis Hasting. Se observan importantes diferencias entre las probabilidades calculadas con MH o grilla en comparación a las probabilidades calculadas con Monte Carlo o integración exacta (estos últimos dos metodos, más precisos).

# Conclusiones

A lo largo del trabajo, se observa que Metropolis Hastings es una variante que permite sacar muestras de distribuciones de probabilidad, incluso cuando no se tiene idea de cómo es exactamente esa distribución, sólo se necesita su ley. Se ha probado en distribuciones unidimensionales y bidimensionales, como las distribuciones de Kumaraswamy y Rosenbrock. Se ha interactuado con diferentes formas de distribución propuesta y se ha observado cómo esta afecta a la eficiencia del algoritmo. Sin embargo, se han identificado algunas debilidades, por ejemplo, su rendimiento puede ser sensible a la elección de la distribución propuesta, lo que puede afectar la eficiencia y la convergencia del algoritmo. Además, puede ser computacionalmente costoso para dimensiones muy altas o distribuciones multimodales.

Es interesante ver a Metropolis Hastings como una herramienta en estadística bayesiana, ya que ayuda a muestrear y/o graficar distribuciones a posteriori. No obstante, se conocen métodos más precisos para obtener muestras de distribuciones complejas, como por ejemplo, Hamiltonian - Monte Carlo.


